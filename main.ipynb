{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Risk Factors in Cryptocurrency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis Project\n",
    "\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0</div>\n",
    "\n",
    "This file requires `pandas`, `datetime`, `numpy`, `wand`, `pdf2image`, `Pillow`, and `math` to run. If one of these imports fails, please install the corresponding library and make sure that you have activated the corresponding virtual environment.\n",
    "\n",
    "The project follows closely the methodology proposed by Liu, Tsyvinski, and Wu (2022) in their paper titled [Common Risk Factors in Cryptocurrency](https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13119). Researchers and practitioners can use this paper to check the results of the paper and perhaps retrieve an updated version of the basic findings. They can also use it as a toolbox to use for other projects or to run an extended analysis including further risk factors. Finally, asset management firm may use this code to assess the risk of their portfolio or to firm anomalies in the returns of cryptocurrencies.\n",
    "\n",
    "For this analysis, I occasionally had to make assumption, for example, regarding the procedure to convert daily to weekly data. This is especially so because the authors of the paper did not provide a detailed enough description of their decisions. There are other, perhabs better ways of doing certain steps and I am always grateful for any feedback that you might provide.\n",
    "\n",
    "The order of the following sections is closely following the structure of the paper. The outline is:\n",
    "* [I. Data](#I.-Data): The files for all data sources can be found in the data folder. The main blockchain trading data is retrieved from CoinGecko (coingecko_data.py). It is advisable to download the cryptocurrency data set in smaller chunks (for example, 100 cryptocurrencies), since the data set is relatively large and takes long to download due to the API limit. The merge_data.py file can then be used to merge all individal cryptocurrency data files into one large file that is supposed to be loaded into the code below. The daily crptocurrency (aka coin) data is converted to weekly returns using the last available prices: $$r_t = \\frac{p_t-p_{t-1}}{p_{t-1}}$$ One can also compute log-returns instead. The definition of the weeks is as follows: The first 7 days of a given year are the first week. The following 50 weeks consist of 7 days each. The last week has either 8 or 9 days (if the year is a leap year). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, datetime, numpy as np, math, time, os\n",
    "pd.options.mode.chained_assignment = None\n",
    "from wand.image import Image\n",
    "# and other modules from the directory\n",
    "import convert_frequency, data.coingecko_data as coingecko_data, merge_data, render\n",
    "\n",
    "# specify the data range for the analysis\n",
    "# in the paper, the authors start on 2014-01-01 due to data availability\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = str(datetime.date.today())\n",
    "\n",
    "# select the path to the directory where you want to store the data\n",
    "data_path = r\"/Users/Marc/Desktop/Past Affairs/Past Universities/SSE Courses/Master Thesis/Data\"\n",
    "\"\"\"\n",
    "# downloading the data from CoinGecko.com and storing it in smaller data subsets at the specified location\n",
    "# the data contains daily prices, market caps, and trading volumes\n",
    "# this step can take up to 2 days due to the API traffic limit\n",
    "# we are also always checking if the subsequent files already exist (/cg_data.csv, /cg_weekly_data.csv, /cg_weekly_returns.csv, /market_weekly_returns.csv)\n",
    "# this helps in case the previous files have been deleted\n",
    "if not os.path.exists(data_path + \"/coingecko\") and not os.path.exists(data_path + \"/cg_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    coingecko_data.retrieve_data(start_date, end_date, path=data_path)\n",
    "else:\n",
    "    print(\"The individual data files already exist.\")\n",
    "\n",
    "# merging the data subsets and storing the result at the specified location\n",
    "# this task also absorbs part of the preprocessing, so it's recommended to run this step in any case \n",
    "# this step can take up to 12 hours\n",
    "if not os.path.exists(data_path + \"/cg_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    merge_data.merge(start_date, end_date, path=data_path)\n",
    "else:\n",
    "    print(\"The data was already merged into a single file.\")\n",
    "\n",
    "# the data was retrieved on 2023-01-13\n",
    "daily_trading_data = pd.read_csv(data_path+\"/cg_data.csv\")\n",
    "\n",
    "# all unique coin IDs\n",
    "coin_ids = pd.unique(daily_trading_data[\"id\"])\n",
    "print(\"There are \" +str(len(coin_ids)) + \" coins in the data set.\")\n",
    "\n",
    "# downloading the data since the conversion process might also take a long time\n",
    "# if the file for weekly data does not already exist\n",
    "if not os.path.exists(data_path + \"/cg_weekly_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    print(\"The data is being transformed into weekly data.\")\n",
    "    # converting the data subset for every coin into weekly frequency\n",
    "    dfs = []\n",
    "    percentage_counter = 1\n",
    "    print(\"The conversion progress is: \")\n",
    "    for coin_id in coin_ids:\n",
    "        # printing the progress \n",
    "        progress = int(len(dfs) / len(coin_ids) * 100)\n",
    "        if progress > percentage_counter:\n",
    "            percentage_counter += 1\n",
    "            print(str(progress) + \"%\")\n",
    "        # get all the data for one coin\n",
    "        coin_daily_trading_data = daily_trading_data[daily_trading_data[\"id\"] == coin_id]\n",
    "        # now we compute the weekly data\n",
    "        # the function weekly_data is designed to perform this transformation for a single coin at a time\n",
    "        # this step takes a long time since the data set has a large size\n",
    "        coin_weekly_data = convert_frequency.weekly_data(coin_daily_trading_data, start_date, end_date, download=False)\n",
    "        dfs.append(coin_weekly_data)\n",
    "        \n",
    "    # combining all dataframes in the dfs list\n",
    "    weekly_trading_data = pd.concat(dfs)\n",
    "    # downloading the data\n",
    "    weekly_trading_data.to_csv(data_path + \"/cg_weekly_data.csv\", index=False)\n",
    "else:\n",
    "    # next, we need to load the data and \"unwrap\" it again\n",
    "    print(\"The data has already been converted.\")\n",
    "    weekly_trading_data = pd.read_csv(data_path + \"/cg_weekly_data.csv\")\n",
    "\n",
    "# downloading the data since the returns computation process might also take a long time\n",
    "# if the file for weekly returns data does not already exist\n",
    "if not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    print(\"The data is being transformed into weekly returns data.\")\n",
    "    # converting the data subset for every coin into weekly frequency\n",
    "    dfs = []\n",
    "    percentage_counter = 1\n",
    "    print(\"The conversion progress is: \")\n",
    "    for coin_id in coin_ids:\n",
    "        # printing the progress \n",
    "        progress = int(len(dfs) / len(coin_ids) * 100)\n",
    "        if progress > percentage_counter:\n",
    "            percentage_counter += 1\n",
    "            print(str(progress) + \"%\")\n",
    "        coin_weekly_prices = weekly_trading_data[weekly_trading_data[\"id\"] == coin_id]\n",
    "        # we are losing the first week, since we do not have a previous week for the first week (first week of 2014)\n",
    "        coin_weekly_returns = [np.nan]\n",
    "        # the indices of the dataframe for the respective coin\n",
    "        indices = list(coin_weekly_prices.index)\n",
    "        if len(coin_weekly_prices) > 2:\n",
    "            for i in range(len(coin_weekly_prices) - 1):\n",
    "                # to retrieve the actual indices\n",
    "                this_week_index = indices[i]\n",
    "                next_week_index = indices[i + 1]\n",
    "                try:\n",
    "                    weekly_return = (coin_weekly_prices[\"price\"][next_week_index] - coin_weekly_prices[\"price\"][this_week_index]) / coin_weekly_prices[\"price\"][this_week_index]\n",
    "                    # alternatively, the log-return can be computed as follows (math.log() is the natural logarithm by default):\n",
    "                    # weekly_log_return = math.log(coin_weekly_prices[\"price\"][i + 1] / coin_weekly_prices[\"price\"][i])\n",
    "                    coin_weekly_returns.append(weekly_return)\n",
    "                except:\n",
    "                    # this exception occurs when either current or future price are NaN\n",
    "                    coin_weekly_returns.append(np.nan)\n",
    "        # adding the return column to the previous date column\n",
    "        coin_weekly_prices[\"return\"] = coin_weekly_returns\n",
    "        dfs.append(coin_weekly_prices)\n",
    "    # combining all dataframes in the dfs list\n",
    "    coins_weekly_returns = pd.concat(dfs)\n",
    "    # downloading the data\n",
    "    coins_weekly_returns.to_csv(data_path + \"/cg_weekly_returns_data.csv\", index=False)\n",
    "else:\n",
    "    # next, we need to load the data and \"unwrap\" it again\n",
    "    print(\"The data has already been transformed into returns data.\")\n",
    "    coins_weekly_returns = pd.read_csv(data_path + \"/cg_weekly_returns.csv\")\n",
    "\n",
    "coins_weekly_returns_old = coins_weekly_returns\n",
    "# storing the data in a dict with keys for the ID\n",
    "coins_weekly_returns = {}\n",
    "for coin_id in coin_ids:\n",
    "    # get all the data for one coin\n",
    "    coins_weekly_returns[coin_id] = coins_weekly_returns_old[coins_weekly_returns_old[\"id\"] == coin_id]\n",
    "\n",
    "# downloading the data since the returns computation process might also take a long time\n",
    "# if the file for weekly returns data does not already exist\n",
    "if not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    # constructing the cryptocurrency market returns\n",
    "    years = []\n",
    "    weeks = []\n",
    "    market_returns = []\n",
    "    included_ids = []\n",
    "    # taking an arbitrary ID to loop through all weeks\n",
    "    for i in coins_weekly_returns[coin_ids[0]].index:\n",
    "        year = coins_weekly_returns[coin_ids[0]][\"year\"][i]\n",
    "        years.append(year)\n",
    "        week = coins_weekly_returns[coin_ids[0]][\"week\"][i]\n",
    "        weeks.append(week)\n",
    "        returns = []\n",
    "        market_caps = []\n",
    "        # to keep track of which and how many coins are included for every week\n",
    "        weekly_included_ids = []\n",
    "        for coin_id in coin_ids:\n",
    "            coin_weekly_returns = coins_weekly_returns[coin_id]\n",
    "            coin_weekly_data = coin_weekly_returns[(coin_weekly_returns[\"year\"] == year) & (coin_weekly_returns[\"week\"] == week)]\n",
    "            # ignoring all NaNs\n",
    "            # the most convenient way to check if no cell value are NaN is by applying .isna().sum().sum()\n",
    "            if coin_weekly_data.isna().sum().sum() == 0:\n",
    "                # the ID is included\n",
    "                weekly_included_ids.append(coin_id)\n",
    "                returns.append(coin_weekly_data[\"return\"])\n",
    "                market_caps.append(coin_weekly_data[\"market_cap\"])\n",
    "        # if all returns are NaN (for example, in the first week of the time period considered)\n",
    "        if len(returns) == 0:\n",
    "            # if no value was added\n",
    "            market_returns.append(np.nan)\n",
    "            included_ids.append(np.nan)\n",
    "        else:\n",
    "            # for every week add the value-weighted market return (the sumproduct of the returns and the market caps divided by the sum of the market caps) and the included coin IDs\n",
    "            weighted_averge = (sum(x * y for x, y in zip(returns, market_caps)) / sum(market_caps)).tolist()[0]\n",
    "            market_returns.append(weighted_averge)\n",
    "            included_ids.append(weekly_included_ids)\n",
    "    market_weekly_returns = pd.DataFrame({\"year\": years, \"week\": weeks, \"average_return\": market_returns, \"included_ids\": included_ids})\n",
    "    # downloading the data\n",
    "    coins_weekly_returns.to_csv(data_path + \"/market_weekly_returns.csv\", index=False)\n",
    "else:\n",
    "    # next, we need to load the data and \"unwrap\" it again\n",
    "    print(\"The data market returns have already been computred.\")\n",
    "    market_weekly_returns = pd.read_csv(data_path + \"/market_weekly_returns.csv\")\n",
    "\n",
    "print(market_weekly_returns.head())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# mock data\n",
    "import pandas as pd, datetime, random\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = \"2023-01-12\"\n",
    "\n",
    "daily_trading_data = pd.DataFrame({\"id\": [], \"date\": [], \"price\": [], \"market_cap\": [], \"total_volume\": []})\n",
    "daily_trading_data[\"id\"] = [\"Check\"] * 90 + [\"Out\"] * 90\n",
    "daily_trading_data[\"date\"] = [\"2014\"] * 10 + [\"2015\"] * 10 + [\"2016\"] * 10 + [\"2017\"] * 10 + [\"2018\"] * 10 + [\"2019\"] * 10 + [\"2020\"] * 10 + [\"2021\"] * 10 + [\"2022\"] * 10 + [\"2014\"] * 10 + [\"2015\"] * 10 + [\"2016\"] * 10 + [\"2017\"] * 10 + [\"2018\"] * 10 + [\"2019\"] * 10 + [\"2020\"] * 10 + [\"2021\"] * 10 + [\"2022\"] * 10\n",
    "daily_trading_data[\"price\"] = random.sample(range(10, 300), 180)\n",
    "daily_trading_data[\"market_cap\"] = random.sample(range(10, 300), 180)\n",
    "daily_trading_data[\"total_volume\"] = random.sample(range(10, 300), 180)\n",
    "\n",
    "market_weekly_returns = pd.DataFrame({\"year\": [], \"week\": [], \"average_return\": [], \"included_ids\": []})\n",
    "market_weekly_returns[\"year\"] = [2014] * 10 + [2015] * 10 + [2016] * 10 + [2017] * 10 + [2018] * 10 + [2019] * 10 + [2020] * 10 + [2021] * 10 + [2022] * 10\n",
    "market_weekly_returns[\"week\"] = random.sample(range(10, 300), 90)\n",
    "returns = random.sample(range(10, 300), 90)\n",
    "returns = [x / 1000 for x in returns]\n",
    "market_weekly_returns[\"average_return\"] = returns\n",
    "market_weekly_returns[\"included_ids\"] = [[\"Test\"]] * 90\n",
    "\n",
    "coins_weekly_returns = {\"ethereum\": None, \"bitcoin\": None, \"ripple\": None, \"other\": None}\n",
    "for coin in coins_weekly_returns.keys():\n",
    "    year = [2014] * 10 + [2015] * 10 + [2016] * 10 + [2017] * 10 + [2018] * 10 + [2019] * 10 + [2020] * 10 + [2021] * 10 + [2022] * 10\n",
    "    week = random.sample(range(10, 300), 90)\n",
    "    price = random.sample(range(10, 300), 90)\n",
    "    market_cap = random.sample(range(10, 300), 90)\n",
    "    volume = random.sample(range(10, 300), 90)\n",
    "    returns = random.sample(range(10, 300), 90)\n",
    "    returns = [x / 1000 for x in returns]\n",
    "\n",
    "    df = pd.DataFrame({\"year\": year, \"week\": week, \"price\": price, \"market_cap\": market_cap, \"volume\": volume, \"return\": returns})\n",
    "    coins_weekly_returns[coin] = df\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "render.render_summary_statistics(start_date, end_date, daily_trading_data, market_weekly_returns, coins_weekly_returns)\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74309577635f6089c540d7d4ca5dc414c3665bebb3159a664b0224275d6afd7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
