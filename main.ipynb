{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Risk Factors in Cryptocurrency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis Project\n",
    "\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0</div>\n",
    "\n",
    "This file requires `pandas`, `numpy`, `wand`, `pdf2image`, `Pillow`, `os`, `matplotlib`, `statsmodels`, `pycoingecko`, `warnings`, `requests`, and `json` to run. If one of these imports fails, please install the corresponding library and make sure that you have activated the corresponding virtual environment.\n",
    "\n",
    "The project follows closely the methodology proposed by Liu, Tsyvinski, and Wu (2022) in their paper titled [Common Risk Factors in Cryptocurrency](https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13119). Researchers and practitioners can use this paper to approximate the results of the paper and perhaps retrieve an updated version of the basic findings. They can also use it as a toolbox to use for other projects or to run an extended analysis including further risk factors. For this purpose, I have also added various additional variables in the folder `\"data_retrieval\"`, that can be included in extended analyses. Finally, asset management firm may use this code to assess the risk of their portfolio or to firm anomalies in the returns of cryptocurrencies.\n",
    "\n",
    "For this analysis, I occasionally had to make assumption, for example, regarding the procedure to convert daily to weekly data. This is especially so because the authors of the paper did not provide a detailed enough description of their decisions. There are other, perhaps better ways of doing certain steps and I am always grateful for any feedback that you might provide. I always explicitly leave comments to explain my assumptions.\n",
    "\n",
    "The order of the following sections resembles the structure of the original paper. The outline is:\n",
    "* <a href=\"#I._Data\">I. Data</a>\n",
    "* <a href=\"#II._Cross-Sectional_Return_Predictor\">II. Cross-Sectional Return Predictors</a>\n",
    "    * <a href=\"#A._Size_Characteristics\">A. Size Characteristics</a>\n",
    "    * <a href=\"#B._Momentum_Characteristics\">B. Momentum Characteristics</a>\n",
    "    * <a href=\"#C._Volume_Characteristics\">C. Volume Characteristics</a>\n",
    "    * <a href=\"#D._Volatility_Characteristics\">D. Volatility Characteristics</a>\n",
    "* <a href=\"#III._Cryptoccurency_Factors\">III. Cryptocurrency Factors</a>\n",
    "\n",
    "This Jupyter Notebook is accompanied by my master's thesis that serves as a comprehensive read-me."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"I._Data\"></a> \n",
    "# I. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os, data_retrieval.coingecko_data as coingecko_data, helpers, data_retrieval.fred_data as fred_data, warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "from wand.image import Image\n",
    "\n",
    "\"\"\"\n",
    "if input(\"Do you want to disable all warning messages? y/n \") in [\"Y\", \"y\"]:\n",
    "    disable_warnings = True\n",
    "else:\n",
    "    disable_warnings = False\n",
    "\"\"\"\n",
    "disable_warnings = True\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# specify the data range for the analysis\n",
    "# in the paper, the authors start on 2014-01-01 due to data availability\n",
    "start_date = pd.to_datetime(\"2014-01-01\")\n",
    "end_date = pd.to_datetime(\"2018-12-31\")\n",
    "# end_date = pd.to_datetime(\"today\")\n",
    "\n",
    "# select the path to the directory where you want to store the data\n",
    "directory = os.getcwd() + \"/data\"\n",
    "\n",
    "# downloading the data from CoinGecko.com and storing it in smaller data subsets at the specified location\n",
    "# the data contains daily prices, market caps, and trading volumes\n",
    "# this step can take up to 2 days due to the API traffic limit\n",
    "# we are also always checking if the subsequent files already exist (/cg_data.csv, /cg_weekly_data.csv, /cg_weekly_returns.csv, /market_weekly_returns.csv)\n",
    "# this helps in case the previous files have been deleted\n",
    "files = [\"/coingecko\", \"/cg_data.csv\", \"/cg_weekly_returns_data.csv\", \"/market_weekly_returns.csv\"]\n",
    "if any(os.path.exists(directory + file) for file in files):\n",
    "    # if at least 1 file already exists\n",
    "    print(\"The individual data files already exist.\")\n",
    "else:\n",
    "    # if the file or the subsequent files do not already exist\n",
    "    coingecko_data.retrieve_data(start_date, end_date, path=directory)\n",
    "\n",
    "# merging the data subsets and storing the result at the specified location\n",
    "# this task also absorbs part of the preprocessing, so it's recommended to run this step in any case \n",
    "# this step can take long\n",
    "if any(os.path.exists(directory + file) for file in files[1:]):\n",
    "    # if at least 1 file already exists\n",
    "    print(\"The data was already merged into a single file.\")  \n",
    "else:\n",
    "    # if the file or the subsequent files do not already exist\n",
    "    helpers.merge_data(path=directory)  \n",
    "\n",
    "# the data was retrieved on 2023-01-13\n",
    "daily_trading_data = pd.read_csv(directory + \"/cg_data.csv\", index_col=[\"date\"])\n",
    "daily_trading_data.index = pd.to_datetime(daily_trading_data.index)\n",
    "# making sure the subdataframe for every coin_id has the same length\n",
    "# when start_date and end_date set above are shorter than the date range in the retrieved data set, the data set is truncated\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "daily_trading_data = daily_trading_data.groupby(\"coin_id\", group_keys=True).apply(lambda x: x.reindex(date_range)).reset_index(level=\"coin_id\", drop=True)\n",
    "# making sure the previously missing values have a value for coin_id\n",
    "daily_trading_data[\"coin_id\"].fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "# downloading the data since the conversion process might also take a long time\n",
    "if any(os.path.exists(directory + file) for file in files[2:]):\n",
    "    # if at least 1 file already exists\n",
    "    print(\"The data has already been converted to weekly frequency.\")\n",
    "    weekly_returns_data = pd.read_csv(directory + \"/cg_weekly_returns_data.csv\", index_col=[\"date\"])\n",
    "    weekly_returns_data.index = pd.to_datetime(weekly_returns_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    weekly_returns_data = weekly_returns_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "else:\n",
    "    # this function converts the frequency and also computes the return series\n",
    "    # this step can take a while\n",
    "    weekly_returns_data = helpers.convert_frequency(daily_trading_data, method=\"last\", returns=True)\n",
    "    # imputing all missing return values by the mean after the first non-missing value\n",
    "    weekly_returns_data[\"return\"] = weekly_returns_data.groupby(\"coin_id\", group_keys=False)[\"return\"].transform(lambda x: x.loc[x.first_valid_index():].fillna(x.mean()))\n",
    "    # imputing all missing market_cap values by the average of the last and next available data values after the first non-missing value\n",
    "    weekly_returns_data[\"market_cap\"] = weekly_returns_data.groupby(\"coin_id\", group_keys=False)[\"market_cap\"].transform(lambda x: x.loc[x.first_valid_index():].fillna((x.fillna(method=\"backfill\") + x.fillna(method=\"ffill\")) / 2))\n",
    "    # some returns are exorbitant (above 10,00%) and need to be removed\n",
    "    weekly_returns_data[\"return\"].where(weekly_returns_data[\"return\"] < 10, np.nan, inplace=True)\n",
    "    # including only cryptocurrencies with a market capitalization of above $1 million on a given date; also exclude the rows where the market capitalization is missing\n",
    "    for var in [\"price\", \"return\", \"total_volume\"]:\n",
    "        weekly_returns_data.loc[((weekly_returns_data[\"market_cap\"].isna()) | (weekly_returns_data[\"market_cap\"] < 1000000)) & ~weekly_returns_data[\"coin_id\"].isin([\"bitcoin\", \"ripple\", \"ethereum\"]), var] = np.nan\n",
    "    # downloading the data\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    weekly_returns_data = weekly_returns_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "    weekly_returns_data[\"coin_id\"].fillna(method=\"ffill\", inplace=True)\n",
    "    weekly_returns_data.to_csv(directory + \"/cg_weekly_returns_data.csv\")\n",
    "\n",
    "# imputing all missing market_cap values by the average of the last and next available data values after the first non-missing value\n",
    "daily_trading_data[\"market_cap\"] = daily_trading_data.groupby(\"coin_id\", group_keys=False)[\"market_cap\"].transform(lambda x: x.loc[x.first_valid_index():].fillna((x.fillna(method=\"backfill\") + x.fillna(method=\"ffill\")) / 2))\n",
    "# including only cryptocurrencies with a market capitalization of above $1 million on a given date; also exclude the rows where the market capitalization is missing\n",
    "daily_trading_data.loc[((daily_trading_data[\"market_cap\"].isna()) | (daily_trading_data[\"market_cap\"] < 1000000)) & ~daily_trading_data[\"coin_id\"].isin([\"bitcoin\", \"ripple\", \"ethereum\"]), \"price\"] = np.nan\n",
    "daily_trading_data.loc[((daily_trading_data[\"market_cap\"].isna()) | (daily_trading_data[\"market_cap\"] < 1000000)) & ~daily_trading_data[\"coin_id\"].isin([\"bitcoin\", \"ripple\", \"ethereum\"]), \"total_volume\"] = np.nan\n",
    "\n",
    "# computing the risk-free rate\n",
    "# a pd dataframe with columns for date and DGS1MO\n",
    "risk_free_rate_daily_data = fred_data.retrieve_data(start_date, end_date, series_ids = [\"DGS1MO\"], download=False)\n",
    "# converting to weekly data before making changes to risk_free_rate_daily_data\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "risk_free_rate = risk_free_rate_daily_data.resample(\"W-SUN\").last().reindex(date_range)\n",
    "# the missing data points always lie on weekends, and hence we impute these values by the average of the Friday and Monday values\n",
    "# computing the average of the Friday and Monday values\n",
    "risk_free_rate_daily_data[\"monday_shifted_to_friday\"] = risk_free_rate_daily_data[\"DGS1MO\"].shift(-3)\n",
    "risk_free_rate_daily_data[\"monday_friday_average\"] = (risk_free_rate_daily_data[\"DGS1MO\"] + risk_free_rate_daily_data[\"monday_shifted_to_friday\"]) / 2\n",
    "# shifting the results from Friday to Saturday\n",
    "risk_free_rate_daily_data[\"monday_friday_average_saturday\"] = risk_free_rate_daily_data[\"monday_friday_average\"].shift(1)\n",
    "# and to Sunday\n",
    "risk_free_rate_daily_data[\"monday_friday_average_sunday\"] = risk_free_rate_daily_data[\"monday_friday_average\"].shift(2)\n",
    "risk_free_rate_daily_data.loc[risk_free_rate_daily_data.index.weekday == 5, \"DGS1MO\"] = risk_free_rate_daily_data[\"monday_friday_average_saturday\"]\n",
    "risk_free_rate_daily_data.loc[risk_free_rate_daily_data.index.weekday == 6, \"DGS1MO\"] = risk_free_rate_daily_data[\"monday_friday_average_sunday\"]\n",
    "# next, we impute all remaining missing data points by the average of the last and next available data values\n",
    "risk_free_rate_daily_data[\"DGS1MO\"] = (risk_free_rate_daily_data[\"DGS1MO\"].fillna(method=\"backfill\") + risk_free_rate_daily_data[\"DGS1MO\"].fillna(method=\"ffill\")) / 2\n",
    "# also making sure the last and first values are set properly in case they are missing\n",
    "risk_free_rate_daily_data[\"DGS1MO\"] = risk_free_rate_daily_data[\"DGS1MO\"].fillna(method=\"backfill\").fillna(method=\"ffill\")\n",
    "\n",
    "# downloading the data since the returns computation process might also take a long time\n",
    "if any(os.path.exists(directory + file) for file in files[3:]):\n",
    "    # if at least 1 file already exists\n",
    "    print(\"The market returns data has already been computed.\")\n",
    "    market_weekly_returns = pd.read_csv(directory + \"/market_weekly_returns.csv\", index_col=[\"date\"])\n",
    "    market_weekly_returns.index = pd.to_datetime(market_weekly_returns.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    market_weekly_returns = market_weekly_returns.reindex(date_range)\n",
    "else:\n",
    "    # Compute the excess weighted average of the returns using the market capitalization as the weight\n",
    "    market_weekly_returns = weekly_returns_data.groupby(weekly_returns_data.index, group_keys=False).apply(lambda x: (x[\"return\"]*x[\"market_cap\"]).sum()/x[\"market_cap\"].sum())\n",
    "    market_weekly_returns.rename(\"market_return\", inplace=True)\n",
    "    market_weekly_returns = market_weekly_returns.to_frame()\n",
    "    market_weekly_returns.index = pd.to_datetime(market_weekly_returns.index)\n",
    "    # imputing all missing values with the mean after the first non-missing value\n",
    "    market_weekly_returns[\"market_return\"] = market_weekly_returns[\"market_return\"].loc[market_weekly_returns[\"market_return\"].first_valid_index():].fillna(market_weekly_returns[\"market_return\"].mean())\n",
    "    # making sure the first return value is missing and not 0\n",
    "    market_weekly_returns.loc[market_weekly_returns.index == market_weekly_returns.index.min(), \"market_return\"] = np.nan\n",
    "    # filling in the missing values with NaNs\n",
    "    date_range = pd.date_range(start=weekly_returns_data.index.min(), end=weekly_returns_data.index.max(), freq=\"W\")\n",
    "    market_weekly_returns = market_weekly_returns.reindex(date_range)\n",
    "    market_weekly_returns.index.name = \"date\"\n",
    "    market_weekly_returns[\"market_excess_return\"] = market_weekly_returns[\"market_return\"].sub(risk_free_rate[\"DGS1MO\"], axis=0)\n",
    "    # downloading the data\n",
    "    market_weekly_returns.to_csv(directory + \"/market_weekly_returns.csv\")\n",
    "\n",
    "\"\"\"\n",
    "# inverting the colors in the PDF in case the user is using dark mode\n",
    "if input(\"Is your editor is dark mode? y/n \") in [\"Y\", \"y\"]:\n",
    "    invert = True\n",
    "else:\n",
    "    invert = False\n",
    "\"\"\"\n",
    "invert = True\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "# we need also the daily data since Panel A is built on daily data\n",
    "# this step takes about 1 hour\n",
    "helpers.render_summary_statistics(daily_data=daily_trading_data, market_weekly_data=market_weekly_returns, weekly_data=weekly_returns_data, invert=invert)\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# plotting the time series graphs\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "\n",
    "bitcoin_weekly_returns = weekly_returns_data[weekly_returns_data[\"coin_id\"] == \"bitcoin\"]\n",
    "ripple_weekly_returns = weekly_returns_data[weekly_returns_data[\"coin_id\"] == \"ripple\"]\n",
    "ethereum_weekly_returns = weekly_returns_data[weekly_returns_data[\"coin_id\"] == \"ethereum\"]\n",
    "\n",
    "year_and_week = (market_weekly_returns.index.isocalendar().year + market_weekly_returns.index.isocalendar().week / 52).tolist()\n",
    "\n",
    "market_weekly_returns[\"market_index\"] = (1 + market_weekly_returns[\"market_excess_return\"]).cumprod()\n",
    "# Set the first value of return_index to 1\n",
    "market_weekly_returns.loc[market_weekly_returns.index == market_weekly_returns.index.min(), \"market_index\"] = 1\n",
    "# imputing missing index values with the last non-missing value\n",
    "market_weekly_returns[\"market_index\"] = market_weekly_returns[\"market_index\"].fillna(method='ffill')\n",
    "market_index = market_weekly_returns[\"market_index\"].tolist()\n",
    "\n",
    "bitcoin_weekly_returns[\"bitcoin_index\"] = (1 + bitcoin_weekly_returns[\"return\"]).cumprod()\n",
    "bitcoin_weekly_returns.loc[bitcoin_weekly_returns.index == bitcoin_weekly_returns.index.min(), \"bitcoin_index\"] = 1\n",
    "bitcoin_weekly_returns[\"bitcoin_index\"] = bitcoin_weekly_returns[\"bitcoin_index\"].fillna(method='ffill')\n",
    "bitcoin_index = bitcoin_weekly_returns[\"bitcoin_index\"].tolist()\n",
    "\n",
    "ripple_weekly_returns[\"ripple_index\"] = (1 + ripple_weekly_returns[\"return\"]).cumprod()\n",
    "ripple_weekly_returns.loc[ripple_weekly_returns.index == ripple_weekly_returns.index.min(), \"ripple_index\"] = 1\n",
    "ripple_weekly_returns[\"ripple_index\"] = ripple_weekly_returns[\"ripple_index\"].fillna(method='ffill')\n",
    "ripple_index = ripple_weekly_returns[\"ripple_index\"].tolist()\n",
    "\n",
    "ethereum_weekly_returns[\"ethereum_index\"] = (1 + ethereum_weekly_returns[\"return\"]).cumprod()\n",
    "ethereum_weekly_returns.loc[ethereum_weekly_returns.index == ethereum_weekly_returns.index.min(), \"ethereum_index\"] = 1\n",
    "ethereum_weekly_returns[\"ethereum_index\"] = ethereum_weekly_returns[\"ethereum_index\"].fillna(method='ffill')\n",
    "ethereum_index = ethereum_weekly_returns[\"ethereum_index\"].tolist()\n",
    "\n",
    "vars = [bitcoin_index, ripple_index, ethereum_index]\n",
    "labels = [\"Bitcoin\", \"Rippel\", \"Ethereum\"]\n",
    "# max = [41, 151, 501]\n",
    "max = [10000, 10000, 10000]\n",
    "step_size = [10, 50, 100]\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(year_and_week, np.array(market_index), color=\"black\", label=\"Coin Market\", linewidth=0.5)\n",
    "    plt.plot(year_and_week, np.array(vars[i]), color=\"black\", linestyle=\"dashed\", label = labels[i], linewidth=0.5)\n",
    "    plt.tick_params(color=\"white\")\n",
    "    # plt.yticks(np.arange(0, max[i], step_size[i]))\n",
    "    plt.xticks(np.arange(start_date.year, end_date.year + 1, 2))\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"$ Value of Investment\")\n",
    "    plt.rcParams['axes.spines.right'] = False\n",
    "    plt.rcParams['axes.spines.top'] = False\n",
    "    plt.legend(frameon=False, prop={\"size\": 10}, loc=\"upper left\", ncol=2)\n",
    "    plt.show()\n",
    "print(\"Figure 1. Cryptocurrency market and major coins. This figure plots the aggregate cryptocurrency market against Bitcoin, Ripple, and Ethereum.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"II._Cross-Sectional_Return_Predictor\"></a>\n",
    "# II. Cross-Sectional Return Predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"A._Size_Characteristics\"></a>\n",
    "## A. Size Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# this steps might take a little longer, so it makes sense to save the data\n",
    "if not os.path.exists(directory + \"/weekly_returns_size_data.csv\"):\n",
    "    # creating the data for the size characteristics in coins_weekly_returns\n",
    "    # adding log_market_cap\n",
    "    weekly_returns_data[\"log_market_cap\"] = np.where(weekly_returns_data[\"market_cap\"] == 0, np.nan, np.log(weekly_returns_data[\"market_cap\"]))\n",
    "    # adding log_price\n",
    "    weekly_returns_data[\"log_price\"] = np.where(weekly_returns_data[\"price\"] == 0, np.nan, np.log(weekly_returns_data[\"price\"]))\n",
    "    # adding log_max_price\n",
    "    # this step might take a while\n",
    "    max_price = helpers.convert_frequency(daily_trading_data[[\"coin_id\", \"price\"]], method=\"max\")[\"price\"]\n",
    "    weekly_returns_data[\"log_max_price\"] = np.where(max_price == 0, np.nan, np.log(max_price))\n",
    "    # adding age as the number of days listed since the first trading day of that respective coin (considering that the time period begins on 2014-01-01)\n",
    "    weekly_returns_data[\"age\"] = weekly_returns_data.groupby(\"coin_id\", group_keys=False)[\"return\"].transform(lambda x: np.maximum(0, (x.index - x.first_valid_index()).days) if x.first_valid_index() is not None else pd.Series([0] * len(x)))\n",
    "    # making sure no dates were lost\n",
    "    date_range = pd.date_range(start=weekly_returns_data.index.min(), end=weekly_returns_data.index.max(), freq=\"W-SUN\")\n",
    "    weekly_returns_data = weekly_returns_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "    weekly_returns_data.to_csv(directory + \"/weekly_returns_size_data.csv\")\n",
    "else:\n",
    "    weekly_returns_data = pd.read_csv(directory + \"/weekly_returns_size_data.csv\", index_col=[\"date\"])\n",
    "    weekly_returns_data.index = pd.to_datetime(weekly_returns_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    weekly_returns_data = weekly_returns_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "# computing the zero-investment long-short strategies based on the size-related characteristics of market capitalization, price, maximum day price, and age\n",
    "if not os.path.exists(directory + \"/quintile_size_data.csv\"):\n",
    "    # \"risk_free_rate\" now is a pd dataframe with columns for date and DGS1MO (the risk-free rate)\n",
    "    quintile_data = risk_free_rate\n",
    "    for size_characteristic in [\"log_market_cap\", \"log_price\", \"log_max_price\", \"age\"]:\n",
    "        quintile_returns_data = helpers.quintile_returns(weekly_returns_data, size_characteristic)\n",
    "        # subtracting the risk-free rate from every column of quintile returns\n",
    "        quintile_returns_data = quintile_returns_data.sub(quintile_data[\"DGS1MO\"], axis=0)\n",
    "        quintile_data = pd.concat([quintile_data, quintile_returns_data], axis=1)\n",
    "        # computing the returns of the long-short strategy\n",
    "        quintile_data[size_characteristic + \"_long_short\"] = np.where(quintile_data[size_characteristic + \"_q1\"].isna(), quintile_data[size_characteristic + \"_q5\"], quintile_data[size_characteristic + \"_q5\"] - quintile_data[size_characteristic + \"_q1\"])\n",
    "\n",
    "    quintile_data.index.name = \"date\"\n",
    "    # downloading the data\n",
    "    quintile_data.to_csv(directory + \"/quintile_size_data.csv\")\n",
    "else:\n",
    "    print(\"The data for the excess long-short strategies for the different size characteristics has already been computed.\")\n",
    "    quintile_data = pd.read_csv(directory + \"/quintile_size_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data.index = pd.to_datetime(quintile_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    quintile_data = quintile_data.reindex(date_range)\n",
    "\n",
    "print(\"Table 2 lists the return predictor definitions and can be found in the paper.\")\n",
    "\n",
    "\"\"\"\n",
    "# inverting the colors in the PDF in case the user is using dark mode\n",
    "if input(\"Is your editor is dark mode? y/n \") in [\"Y\", \"y\"]:\n",
    "    invert = True\n",
    "else:\n",
    "    invert = False\n",
    "\"\"\"\n",
    "invert = True\n",
    "\n",
    "# use \"\\symbol{37}\" in the latex file for the percentage sign\n",
    "helpers.render_quintiles(quintile_data, \"latex_templates/size_quintiles.tex\", [\"log_market_cap\", \"log_price\", \"log_max_price\", \"age\"], invert)\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"B._Momentum_Characteristics\"></a>\n",
    "## B. Momentum Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# 1-week past returns\n",
    "weekly_returns_data[\"one_week_momentum\"] = weekly_returns_data[\"return\"]\n",
    "# 2-week past returns\n",
    "# here, the transition from one data set to another is not an issue since the variable is always NaN when the return in the same week is NaN\n",
    "weekly_returns_data[\"two_week_momentum\"] = (1 + weekly_returns_data[\"return\"]).rolling(2).apply(lambda x: x.prod() - 1, raw=True)\n",
    "# 3-week past returns\n",
    "weekly_returns_data[\"three_week_momentum\"] = (1 + weekly_returns_data[\"return\"]).rolling(3).apply(lambda x: x.prod() - 1, raw=True)\n",
    "# 4-week past returns\n",
    "weekly_returns_data[\"four_week_momentum\"] = (1 + weekly_returns_data[\"return\"]).rolling(4).apply(lambda x: x.prod() - 1, raw=True)\n",
    "# 1-to-4-week past returns\n",
    "# applying the transformation to every coin ID subset separately to avoid complications at the transitions between 2 coin IDs \n",
    "weekly_returns_data[\"one_to_four_week_momentum\"] = weekly_returns_data.groupby(\"coin_id\", group_keys=False)[\"return\"].transform(lambda x: (1 + x).shift(1).rolling(3).apply(lambda x: x.prod() - 1, raw=True))\n",
    "# 8-week past returns\n",
    "weekly_returns_data[\"eight_week_momentum\"] = (1 + weekly_returns_data[\"return\"]).rolling(8).apply(lambda x: x.prod() - 1, raw=True)\n",
    "# 16-week past returns\n",
    "weekly_returns_data[\"sixteen_week_momentum\"] = (1 + weekly_returns_data[\"return\"]).rolling(16).apply(lambda x: x.prod() - 1, raw=True)\n",
    "# 50-week past returns\n",
    "weekly_returns_data[\"fifty_week_momentum\"] = (1 + weekly_returns_data[\"return\"]).rolling(50).apply(lambda x: x.prod() - 1, raw=True)\n",
    "# 100-week past returns\n",
    "weekly_returns_data[\"one_hundred_week_momentum\"] = (1 + weekly_returns_data[\"return\"]).rolling(100).apply(lambda x: x.prod() - 1, raw=True)\n",
    "# constructing the return quintiles for the zero-investment long-short strategies based on past 1-, 2-, 3-, 4-, 1-to-4-, 8-, 16-, 50-, and 8-week returns\n",
    "\n",
    "# downloading the data since the returns computation process might also take a long time\n",
    "# if the file for weekly returns data does not already exist\n",
    "if not os.path.exists(directory + \"/quintile_momentum_data.csv\"):\n",
    "\n",
    "    # \"risk_free_rate\" now is a pd dataframe with columns for year, month, and DGS1MO (the risk-free rate)\n",
    "    quintile_data = risk_free_rate\n",
    "    for momentum_return_series in [\"one_week_momentum\", \"two_week_momentum\", \"three_week_momentum\", \"four_week_momentum\", \"one_to_four_week_momentum\", \"eight_week_momentum\", \"sixteen_week_momentum\", \"fifty_week_momentum\", \"one_hundred_week_momentum\"]:\n",
    "        # here we can use the quintile function for the past returns\n",
    "        quintile_returns_data = helpers.quintile_returns(weekly_returns_data, momentum_return_series)\n",
    "        # subtracting the risk-free rate from every column of quintile returns\n",
    "        quintile_returns_data = quintile_returns_data.sub(quintile_data[\"DGS1MO\"], axis=0)\n",
    "        quintile_data = pd.concat([quintile_data, quintile_returns_data], axis=1)\n",
    "        # computing the return of the long-short strategy\n",
    "        quintile_data[momentum_return_series + \"_long_short\"] = np.where(quintile_data[momentum_return_series + \"_q1\"].isna(), quintile_data[momentum_return_series + \"_q5\"], quintile_data[momentum_return_series + \"_q5\"] - quintile_data[momentum_return_series + \"_q1\"])\n",
    "\n",
    "    quintile_data.index.name = \"date\"\n",
    "    # downloading the data\n",
    "    quintile_data.to_csv(directory + \"/quintile_momentum_data.csv\")\n",
    "else:\n",
    "    print(\"The data for the excess long-short strategies for the different momentum strategies has already been computed.\")\n",
    "    quintile_data = pd.read_csv(directory + \"/quintile_momentum_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data.index = pd.to_datetime(quintile_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    quintile_data = quintile_data.reindex(date_range)\n",
    "\n",
    "\"\"\"\n",
    "# inverting the colors in the PDF in case the user is using dark mode\n",
    "if input(\"Is your editor is dark mode? y/n \") in [\"Y\", \"y\"]:\n",
    "    invert = True\n",
    "else:\n",
    "    invert = False\n",
    "\"\"\"\n",
    "invert = True\n",
    "\n",
    "helpers.render_quintiles(quintile_data, \"latex_templates/momentum_quintiles.tex\", [\"one_week_momentum\", \"two_week_momentum\", \"three_week_momentum\", \"four_week_momentum\", \"one_to_four_week_momentum\", \"eight_week_momentum\", \"sixteen_week_momentum\", \"fifty_week_momentum\", \"one_hundred_week_momentum\"], invert)\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"C._Volume_Characteristics\"></a>\n",
    "## C. Volume Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# this steps might take a little longer, so it makes sense to save the data\n",
    "if not os.path.exists(directory + \"/weekly_returns_volume_data.csv\"):\n",
    "    # adding average_volume\n",
    "    average_volume = helpers.convert_frequency(daily_trading_data[[\"coin_id\", \"total_volume\"]], method=\"mean\")[\"total_volume\"]\n",
    "    weekly_returns_data[\"log_average_volume\"] = np.where(average_volume == 0, np.nan, np.log(average_volume))\n",
    "    # adding price_times_volume\n",
    "    weekly_returns_data[\"price_times_volume\"] = weekly_returns_data[\"log_average_volume\"] * weekly_returns_data[\"price\"]\n",
    "    # adding price_times_volume_scaled_market_cap\n",
    "    weekly_returns_data[\"price_times_volume_scaled_market_cap\"] = weekly_returns_data[\"price_times_volume\"] / weekly_returns_data[\"market_cap\"]\n",
    "    weekly_returns_data.to_csv(directory + \"/weekly_returns_volume_data.csv\")\n",
    "else:\n",
    "    weekly_returns_data = pd.read_csv(directory + \"/weekly_returns_volume_data.csv\", index_col=[\"date\"])\n",
    "    weekly_returns_data.index = pd.to_datetime(weekly_returns_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    weekly_returns_data = weekly_returns_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "\n",
    "if not os.path.exists(directory + \"/quintile_volume_data.csv\"):\n",
    "    \n",
    "    # \"risk_free_rate\" now is a pd dataframe with columns for date and DGS1MO (the risk-free rate)\n",
    "    quintile_data = risk_free_rate\n",
    "    for volume_measure in [\"log_average_volume\", \"price_times_volume\", \"price_times_volume_scaled_market_cap\"]:\n",
    "        # here we can use the quintile function for the past returns\n",
    "        quintile_returns_data = helpers.quintile_returns(weekly_returns_data, volume_measure)\n",
    "        # subtracting the risk-free rate from every column of quintile returns\n",
    "        quintile_returns_data = quintile_returns_data.sub(quintile_data[\"DGS1MO\"], axis=0)\n",
    "        quintile_data = pd.concat([quintile_data, quintile_returns_data], axis=1)\n",
    "        # computing the returns of the long-short strategy\n",
    "        quintile_data[volume_measure + \"_long_short\"] = np.where(quintile_data[volume_measure + \"_q1\"].isna(), quintile_data[volume_measure + \"_q5\"], quintile_data[volume_measure + \"_q5\"] - quintile_data[volume_measure + \"_q1\"])\n",
    "    \n",
    "    quintile_data.index.name = \"date\"\n",
    "    # downloading the data\n",
    "    quintile_data.to_csv(directory + \"/quintile_volume_data.csv\")\n",
    "else:\n",
    "    print(\"The data for the excess long-short strategies for the different size characteristics has already been computed.\")\n",
    "    quintile_data = pd.read_csv(directory + \"/quintile_volume_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data.index = pd.to_datetime(quintile_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    quintile_data = quintile_data.reindex(date_range)\n",
    "\n",
    "\"\"\"\n",
    "# inverting the colors in the PDF in case the user is using dark mode\n",
    "if input(\"Is your editor is dark mode? y/n \") in [\"Y\", \"y\"]:\n",
    "    invert = True\n",
    "else:\n",
    "    invert = False\n",
    "\"\"\"\n",
    "invert = True\n",
    "\n",
    "helpers.render_quintiles(quintile_data, \"latex_templates/volume_quintiles.tex\", [\"log_average_volume\", \"price_times_volume\", \"price_times_volume_scaled_market_cap\"], invert)\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"D._Volatility_Characteristics\"></a>\n",
    "## D. Volatility Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "# this steps might take a little longer, so it makes sense to save the data\n",
    "if not os.path.exists(directory + \"/weekly_returns_volatility_data.csv\"):\n",
    "    # the same goes for the daily data\n",
    "    if not os.path.exists(directory + \"/daily_trading_volatility_data.csv\"):\n",
    "\n",
    "        # computing the returns of the daily data\n",
    "        daily_trading_data[\"return\"] = daily_trading_data.groupby(\"coin_id\", group_keys=False)[\"price\"].pct_change()\n",
    "        # imputing all missing values with the mean after the first non-missing value\n",
    "        daily_trading_data[\"return\"] = daily_trading_data.groupby(\"coin_id\", group_keys=False)[\"return\"].transform(lambda x: x.loc[x.first_valid_index():].fillna(x.mean()))\n",
    "        # computing the excess returns of the daily data\n",
    "        number_unique_coins = daily_trading_data[\"coin_id\"].nunique()\n",
    "        daily_trading_data = daily_trading_data.assign(risk_free_rate=np.tile(risk_free_rate_daily_data[\"DGS1MO\"].values, number_unique_coins))\n",
    "        daily_trading_data = daily_trading_data.assign(excess_return=daily_trading_data[\"return\"] - daily_trading_data[\"risk_free_rate\"])\n",
    "        # computing the daily excess crypto market factor\n",
    "        market_return = daily_trading_data.groupby(daily_trading_data.index, group_keys=False).apply(lambda x: (x[\"return\"]*x[\"market_cap\"]).sum()/x[\"market_cap\"].sum())\n",
    "        daily_trading_data = daily_trading_data.assign(market_return=np.tile(market_return.values, number_unique_coins))\n",
    "        # imputing all missing values with the mean after the first non-missing value\n",
    "        daily_trading_data[\"market_return\"] = daily_trading_data.groupby(\"coin_id\", group_keys=False)[\"market_return\"].transform(lambda x: x.loc[x.first_valid_index():].fillna(x.mean()))\n",
    "        daily_trading_data = daily_trading_data.assign(market_excess_return=daily_trading_data[\"market_return\"] - daily_trading_data[\"risk_free_rate\"])\n",
    "        # computing the market return factor lagged by 1 day\n",
    "        daily_trading_data = daily_trading_data.assign(market_lagged_one_day=np.tile(market_return.shift(1).values, number_unique_coins))\n",
    "        daily_trading_data = daily_trading_data.assign(risk_free_lagged_one_day=np.tile(risk_free_rate_daily_data[\"DGS1MO\"].shift(1).values, number_unique_coins))\n",
    "        daily_trading_data = daily_trading_data.assign(market_lagged_one_day=daily_trading_data[\"market_lagged_one_day\"] - daily_trading_data[\"risk_free_lagged_one_day\"])\n",
    "        # computing the market return facotr lagged by 2 days\n",
    "        daily_trading_data = daily_trading_data.assign(market_lagged_two_days=np.tile(market_return.shift(2).values, number_unique_coins))\n",
    "        daily_trading_data = daily_trading_data.assign(risk_free_lagged_two_days=np.tile(risk_free_rate_daily_data[\"DGS1MO\"].shift(2).values, number_unique_coins))\n",
    "        daily_trading_data = daily_trading_data.assign(market_lagged_two_days=daily_trading_data[\"market_lagged_two_days\"] - daily_trading_data[\"risk_free_lagged_two_days\"])\n",
    "        # filling in the missing values with last known value\n",
    "        date_range = pd.date_range(start=daily_trading_data.index.min(), end=daily_trading_data.index.max(), freq=\"D\")\n",
    "        daily_trading_data = daily_trading_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "        daily_trading_data.index.name = \"date\"\n",
    "\n",
    "        # replacing volumes of 0 with NaN to avoid division by 0; values of 0 are very likely missing values anyways\n",
    "        daily_trading_data[\"total_volume\"].replace([0, 0.0], np.nan)\n",
    "        daily_trading_data.to_csv(directory + \"/daily_trading_volatility_data.csv\")\n",
    "    else:\n",
    "        daily_trading_data = pd.read_csv(directory + \"/daily_trading_volatility_data.csv\", index_col=[\"date\"])\n",
    "        daily_trading_data.index = pd.to_datetime(daily_trading_data.index)\n",
    "        # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "        daily_trading_data = daily_trading_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "\n",
    "    print(\"The computation progress for the weekly data series is:\")\n",
    "    percentage_counter = 0\n",
    "    list_length = len(daily_trading_data[\"coin_id\"].unique())\n",
    "    counter = 0\n",
    "    dict = {\"beta\": [], \"residual_std\": [], \"r_squared_difference\": [], \"standard_deviation\": [], \"max_return\": [], \"volume_std\": [], \"abs_return_per_volume\": []}\n",
    "\n",
    "    for coin_id in daily_trading_data[\"coin_id\"].unique():\n",
    "        sub_df = daily_trading_data[daily_trading_data[\"coin_id\"] == coin_id].copy()\n",
    "        # printing the progress\n",
    "        counter += 1\n",
    "        progress = int(counter / list_length * 100)\n",
    "        if progress > percentage_counter:\n",
    "            percentage_counter += 1\n",
    "            print(str(progress) + \"%\")\n",
    "        for index in sub_df.index:\n",
    "            # converting to weekly data\n",
    "            # 6 stands for Sunday\n",
    "            # the process might take a long time\n",
    "            if index.dayofweek == 6:\n",
    "                # computing the linear model for the last 365 days of data\n",
    "                days = 365\n",
    "                # sliding_window_view return the rolling window for all indices but we are only interested in the last day, so we use [-1]\n",
    "                # for the regression, we drop the missing values\n",
    "                sub_df_nas_dropped = sub_df[sub_df.index <= index][[\"excess_return\", \"market_excess_return\", \"market_lagged_one_day\", \"market_lagged_two_days\"]].copy().dropna()\n",
    "                if len(sub_df_nas_dropped) < 365 or sub_df_nas_dropped.empty:\n",
    "                    dict[\"beta\"].append(np.nan)\n",
    "                    dict[\"residual_std\"].append(np.nan)\n",
    "                    dict[\"r_squared_difference\"].append(np.nan)\n",
    "                else:    \n",
    "                    # this step is very computationally intensive\n",
    "                    model = sm.OLS(sliding_window_view(sub_df_nas_dropped[\"market_excess_return\"], days)[-1], sm.add_constant(sliding_window_view(sub_df_nas_dropped[\"excess_return\"], days)[-1])).fit()\n",
    "                    dict[\"beta\"].append(model.params[1])\n",
    "                    # computing the regression residuals\n",
    "                    # this step might be unpractical since computing all residuals is very computationally expensive\n",
    "                    dict[\"residual_std\"].append(model.resid.std())\n",
    "                    new_model = sm.OLS(sliding_window_view(sub_df_nas_dropped[\"market_excess_return\"], days)[-1], sm.add_constant(pd.DataFrame(data=[sliding_window_view(sub_df_nas_dropped[\"excess_return\"], days)[-1], sliding_window_view(sub_df_nas_dropped[\"market_lagged_one_day\"], days)[-1], sliding_window_view(sub_df_nas_dropped[\"market_lagged_two_days\"], days)[-1]]).T)).fit()\n",
    "                    dict[\"r_squared_difference\"].append(new_model.rsquared - model.rsquared)\n",
    "                # manual computation of beta\n",
    "                # dict[\"beta\"].append(sub_df_nas_dropped[[\"excess_return\", \"market_excess_return\"]].rolling(days).cov().unstack()[\"excess_return\"][\"market_excess_return\"].tolist()[-1] / sub_df_nas_dropped[\"market_excess_return\"].rolling(days).var().tolist()[-1])\n",
    "                # we just consider the return in the portfolio formation week (Monday to Sunday) for the computation of the following variables\n",
    "                days = min(len(sub_df[sub_df.index <= index]), 7)\n",
    "                dict[\"standard_deviation\"].append(sub_df[sub_df.index <= index][\"return\"].rolling(days).std().tolist()[-1])\n",
    "                dict[\"max_return\"].append(sub_df[sub_df.index <= index][\"return\"].rolling(days).max().tolist()[-1])\n",
    "                dict[\"volume_std\"].append(sub_df[sub_df.index <= index][\"total_volume\"].rolling(days).std().tolist()[-1])\n",
    "                absolute_daily_returns = np.abs(sliding_window_view(sub_df[sub_df.index <= index][\"return\"], days)[-1]) / sliding_window_view(sub_df[sub_df.index <= index][\"total_volume\"], days)[-1]\n",
    "                dict[\"abs_return_per_volume\"].append(np.nanmean(absolute_daily_returns) if absolute_daily_returns.size != 0 else np.nan)\n",
    "\n",
    "    weekly_returns_data[\"beta\"] = dict[\"beta\"]\n",
    "    weekly_returns_data[\"residual_std\"] = dict[\"residual_std\"]\n",
    "    weekly_returns_data[\"r_squared_difference\"] = dict[\"r_squared_difference\"]\n",
    "    weekly_returns_data[\"standard_deviation\"] = dict[\"standard_deviation\"]\n",
    "    weekly_returns_data[\"max_return\"] = dict[\"max_return\"]\n",
    "    weekly_returns_data[\"log_volume_std\"] = np.where(dict[\"volume_std\"] == 0, np.nan, np.log(dict[\"volume_std\"]))\n",
    "    weekly_returns_data[\"abs_return_per_volume\"] = dict[\"abs_return_per_volume\"]\n",
    "    # computing beta_squared\n",
    "    weekly_returns_data[\"beta_squared\"] = weekly_returns_data[\"beta\"] ** 2\n",
    "    weekly_returns_data.index.name = \"date\"\n",
    "    weekly_returns_data.to_csv(directory + \"/weekly_returns_volatility_data.csv\")\n",
    "else:\n",
    "    weekly_returns_data = pd.read_csv(directory + \"/weekly_returns_volatility_data.csv\", index_col=[\"date\"])\n",
    "    weekly_returns_data.index = pd.to_datetime(weekly_returns_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    weekly_returns_data = weekly_returns_data.groupby(\"coin_id\", group_keys=False).apply(lambda x: x.reindex(date_range))\n",
    "    weekly_returns_data.index.name = \"date\"\n",
    "\n",
    "if not os.path.exists(directory + \"/quintile_volatility_data.csv\"):\n",
    "\n",
    "    quintile_data = risk_free_rate\n",
    "    volatility_measures = [\"beta\", \"standard_deviation\", \"max_return\", \"log_volume_std\", \"abs_return_per_volume\", \"beta_squared\", \"residual_std\", \"r_squared_difference\"]\n",
    "    for volatility_measure in volatility_measures:\n",
    "        # here we can use the quintile function for the past returns\n",
    "        quintile_returns_data = helpers.quintile_returns(weekly_returns_data, volatility_measure)\n",
    "        # subtracting the risk-free rate from every column of quintile returns\n",
    "        quintile_returns_data = quintile_returns_data.sub(quintile_data[\"DGS1MO\"], axis=0)\n",
    "        quintile_data = pd.concat([quintile_data, quintile_returns_data], axis=1)\n",
    "        # computing the returns of the long-short strategy\n",
    "        quintile_data[volatility_measure + \"_long_short\"] = np.where(quintile_data[volatility_measure + \"_q1\"].isna(), quintile_data[volatility_measure + \"_q5\"], quintile_data[volatility_measure + \"_q5\"] - quintile_data[volatility_measure + \"_q1\"])\n",
    "    \n",
    "    quintile_data.index.name = \"date\"\n",
    "    # downloading the data\n",
    "    quintile_data.to_csv(directory + \"/quintile_volatility_data.csv\")\n",
    "else:\n",
    "    print(\"The data for the excess long-short strategies for the different volatility strategies has already been computed.\")\n",
    "    quintile_data = pd.read_csv(directory + \"/quintile_volatility_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data.index = pd.to_datetime(quintile_data.index)\n",
    "    # truncating the data set in case it is longer than the period set by start_date and end_date\n",
    "    date_range = pd.date_range(start=start_date, end=end_date, freq=\"W-SUN\")\n",
    "    quintile_data = quintile_data.reindex(date_range)\n",
    "\n",
    "\"\"\"\n",
    "# inverting the colors in the PDF in case the user is using dark mode\n",
    "if input(\"Is your editor is dark mode? y/n \") in [\"Y\", \"y\"]:\n",
    "    invert = True\n",
    "else:\n",
    "    invert = False\n",
    "\"\"\"\n",
    "invert = True\n",
    "\n",
    "volatility_measures = [\"beta\", \"standard_deviation\", \"max_return\", \"log_volume_std\", \"abs_return_per_volume\", \"beta_squared\", \"residual_std\", \"r_squared_difference\"]\n",
    "helpers.render_quintiles(quintile_data, \"latex_templates/volatility_quintiles.tex\", volatility_measures, invert)\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"III._Cryptoccurency_Factors\"></a>\n",
    "# III. Cryptocurrency Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# here, we render the table for the one-factor model\n",
    "if not os.path.exists(directory + \"/one_factor_model_data.csv\"):\n",
    "\n",
    "    # quintile data to constract the dependent variables\n",
    "    quintile_data1 = pd.read_csv(directory + \"/quintile_size_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data2 = pd.read_csv(directory + \"/quintile_momentum_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data3 = pd.read_csv(directory + \"/quintile_volume_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data4 = pd.read_csv(directory + \"/quintile_volatility_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data = pd.concat([quintile_data1, quintile_data2, quintile_data3, quintile_data4], axis=1)\n",
    "    quintile_data.index = pd.to_datetime(quintile_data.index)\n",
    "\n",
    "    all_vars = [\"log_market_cap\", \"log_price\", \"log_max_price\", \"age\", \"one_week_momentum\", \"two_week_momentum\", \"three_week_momentum\", \"four_week_momentum\", \"one_to_four_week_momentum\", \"eight_week_momentum\", \"sixteen_week_momentum\", \"fifty_week_momentum\", \"one_hundred_week_momentum\", \"log_average_volume\", \"price_times_volume\", \"price_times_volume_scaled_market_cap\", \"beta\", \"standard_deviation\", \"max_return\", \"log_volume_std\", \"abs_return_per_volume\", \"beta_squared\", \"residual_std\", \"r_squared_difference\"]\n",
    "    alphas = []\n",
    "    t_alphas = []\n",
    "    p_alphas = []\n",
    "    betas = []\n",
    "    t_betas = []\n",
    "    p_betas = []\n",
    "    r_squareds = []\n",
    "    mean_absolute_errors = []\n",
    "    average_r_squareds = []\n",
    "\n",
    "    # create one single dataframe to consistently remove the rows with missing values\n",
    "    df = pd.concat([quintile_data, market_weekly_returns], axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    for var in all_vars:\n",
    "        # the long-short strategy in excess of the risk-free rate is the dependent variable\n",
    "        sub_df = df[[var + \"_long_short\", \"DGS1MO\", \"market_excess_return\"]].copy().dropna()\n",
    "        model = sm.OLS(sub_df[var + \"_long_short\"] - sub_df[\"DGS1MO\"], sm.add_constant(sub_df[\"market_excess_return\"])).fit()\n",
    "        alphas.append(model.params[0])\n",
    "        t_alphas.append(model.tvalues[0])\n",
    "        p_alphas.append(model.pvalues[0])\n",
    "        betas.append(model.params[1])\n",
    "        t_betas.append(model.tvalues[1])\n",
    "        p_betas.append(model.pvalues[1])\n",
    "        r_squareds.append(model.rsquared)\n",
    "        # predict using the fitted model\n",
    "        pred = model.predict(sm.add_constant(sub_df[\"market_excess_return\"]))\n",
    "        # calculating the MAE\n",
    "        MAE = np.mean(np.abs(sub_df[var + \"_long_short\"] - sub_df[\"DGS1MO\"] - pred))\n",
    "        mean_absolute_errors.append(MAE)\n",
    "        # computing the average R-squared for the 5 quintiles excess return series as dependent variables\n",
    "        sum_r_squared = 0\n",
    "        for quintile in [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\"]:\n",
    "            sub_df = df[[var + \"_\" + quintile, \"market_excess_return\"]].copy().dropna()\n",
    "            model = sm.OLS(sub_df[var + \"_\" + quintile], sm.add_constant(sub_df[\"market_excess_return\"])).fit()\n",
    "            sum_r_squared += model.rsquared\n",
    "        average_r_squareds.append(sum_r_squared / 5)\n",
    "    \n",
    "    df = pd.DataFrame({\"ls_strategy\": all_vars, \"alpha\": alphas, \"t_alpha\": t_alphas, \"p_alpha\": p_alphas, \"beta\": betas, \"t_beta\": t_betas, \"p_beta\": p_betas, \"r_squared\": r_squareds, \"mean_absolute_error\": mean_absolute_errors, \"average_r_squared\": average_r_squareds})\n",
    "    # downloading the data\n",
    "    df.to_csv(directory + \"/one_factor_model_data.csv\")\n",
    "else:\n",
    "    print(\"The data for the excess long-short strategies for the different volatility strategies has already been computed.\")\n",
    "    df = pd.read_csv(directory + \"/one_factor_model_data.csv\")\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "\"\"\"\n",
    "# inverting the colors in the PDF in case the user is using dark mode\n",
    "if input(\"Is your editor is dark mode? y/n \") in [\"Y\", \"y\"]:\n",
    "    invert = True\n",
    "else:\n",
    "    invert = False\n",
    "\"\"\"\n",
    "invert = True\n",
    "\n",
    "helpers.render_factor_models_statistics(df, \"latex_templates/one_factor_models_statistics.tex\", invert)\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "if disable_warnings:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# here, we render the table for the multi-factor models\n",
    "# we can use the tertile function for the past returns\n",
    "tertiles_market_cap = helpers.tertile_returns(weekly_returns_data, \"market_cap\")\n",
    "# computing the long-short strategy\n",
    "market_weekly_returns[\"small_minus_big\"] = np.where(tertiles_market_cap[\"market_cap_q1\"].isna(), tertiles_market_cap[\"market_cap_q3\"], tertiles_market_cap[\"market_cap_q3\"] - tertiles_market_cap[\"market_cap_q1\"])\n",
    "\n",
    "# first dividing by market_cap and then by three_week_momentum\n",
    "q1_return_df, q2_return_df = helpers.two_times_three_returns(weekly_returns_data)\n",
    "market_weekly_returns[\"momentum\"] = 0.5 * (q1_return_df[\"three_week_momentum_q3\"] - q2_return_df[\"three_week_momentum_q3\"]) + 0.5 * (q1_return_df[\"three_week_momentum_q1\"] - q2_return_df[\"three_week_momentum_q1\"])\n",
    "\n",
    "if not os.path.exists(directory + \"/multi_factor_models_data.csv\"):\n",
    "\n",
    "    # quintile data to con\n",
    "    quintile_data1 = pd.read_csv(directory + \"/quintile_size_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data2 = pd.read_csv(directory + \"/quintile_momentum_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data3 = pd.read_csv(directory + \"/quintile_volume_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data4 = pd.read_csv(directory + \"/quintile_volatility_data.csv\", index_col=[\"date\"])\n",
    "    quintile_data = pd.concat([quintile_data1, quintile_data2, quintile_data3, quintile_data4], axis=1)\n",
    "    quintile_data.index = pd.to_datetime(quintile_data.index)\n",
    "\n",
    "    all_vars = [\"log_market_cap\", \"log_price\", \"log_max_price\", \"age\", \"one_week_momentum\", \"two_week_momentum\", \"three_week_momentum\", \"four_week_momentum\", \"one_to_four_week_momentum\", \"eight_week_momentum\", \"sixteen_week_momentum\", \"fifty_week_momentum\", \"one_hundred_week_momentum\", \"log_average_volume\", \"price_times_volume\", \"price_times_volume_scaled_market_cap\", \"beta\", \"standard_deviation\", \"max_return\", \"log_volume_std\", \"abs_return_per_volume\", \"beta_squared\", \"residual_std\", \"r_squared_difference\"]\n",
    "\n",
    "    models = []\n",
    "    variables = []\n",
    "    alphas = []\n",
    "    t_alphas = []\n",
    "    p_alphas = []\n",
    "    betas_market_excess_return = []\n",
    "    t_betas_market_excess_return = []\n",
    "    p_betas_market_excess_return = []\n",
    "    betas_small_minus_big = []\n",
    "    t_betas_small_minus_big = []\n",
    "    p_betas_small_minus_big = []\n",
    "    betas_momentum = []\n",
    "    t_betas_momentum = []\n",
    "    p_betas_momentum = []\n",
    "    r_squareds = []\n",
    "    mean_absolute_errors = []\n",
    "    average_r_squareds = []\n",
    "\n",
    "    # create one single dataframe to consistently remove the rows with missing values\n",
    "    df = pd.concat([quintile_data, market_weekly_returns], axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    for var in all_vars:\n",
    "        sub_df1 = df[[var + \"_long_short\", \"DGS1MO\", \"market_excess_return\", \"small_minus_big\"]].copy().dropna()\n",
    "        sub_df2 = df[[var + \"_long_short\", \"DGS1MO\", \"market_excess_return\", \"momentum\"]].copy().dropna()\n",
    "        sub_df3 = df[[var + \"_long_short\", \"DGS1MO\", \"market_excess_return\", \"small_minus_big\", \"momentum\"]].copy().dropna()\n",
    "        # the long-short strategy in excess of the risk-free rate is the dependent variable\n",
    "        # now we have 2 or 3 explanatory variables, respectively\n",
    "        models_dict = {}\n",
    "        models_dict[\"model1\"] = sm.OLS(sub_df1[var + \"_long_short\"] - sub_df1[\"DGS1MO\"], sm.add_constant(sub_df1[[\"market_excess_return\", \"small_minus_big\"]])).fit()\n",
    "        models_dict[\"model2\"] = sm.OLS(sub_df2[var + \"_long_short\"] - sub_df2[\"DGS1MO\"], sm.add_constant(sub_df2[[\"market_excess_return\", \"momentum\"]])).fit()\n",
    "        models_dict[\"model3\"] = sm.OLS(sub_df3[var + \"_long_short\"] - sub_df3[\"DGS1MO\"], sm.add_constant(sub_df3[[\"market_excess_return\", \"small_minus_big\", \"momentum\"]])).fit()\n",
    "\n",
    "        for name in models_dict.keys():\n",
    "            model = models_dict[name]\n",
    "            \n",
    "            models.append(name)\n",
    "            variables.append(var)\n",
    "            alphas.append(model.params[0])\n",
    "            t_alphas.append(model.tvalues[0])\n",
    "            p_alphas.append(model.pvalues[0])\n",
    "            betas_market_excess_return.append(model.params[1])\n",
    "            t_betas_market_excess_return.append(model.tvalues[1])\n",
    "            p_betas_market_excess_return.append(model.pvalues[1])\n",
    "            if name == \"model1\":\n",
    "                betas_small_minus_big.append(model.params[1])\n",
    "                t_betas_small_minus_big.append(model.tvalues[1])\n",
    "                p_betas_small_minus_big.append(model.pvalues[1])\n",
    "                betas_momentum.append(np.nan)\n",
    "                t_betas_momentum.append(np.nan)\n",
    "                p_betas_momentum.append(np.nan)\n",
    "            elif name == \"model2\":\n",
    "                betas_small_minus_big.append(np.nan)\n",
    "                t_betas_small_minus_big.append(np.nan)\n",
    "                p_betas_small_minus_big.append(np.nan)\n",
    "                betas_momentum.append(model.params[1])\n",
    "                t_betas_momentum.append(model.tvalues[1])\n",
    "                p_betas_momentum.append(model.pvalues[1])\n",
    "            elif name == \"model3\":\n",
    "                betas_small_minus_big.append(model.params[1])\n",
    "                t_betas_small_minus_big.append(model.tvalues[1])\n",
    "                p_betas_small_minus_big.append(model.pvalues[1])\n",
    "                betas_momentum.append(model.params[2])\n",
    "                t_betas_momentum.append(model.tvalues[2])\n",
    "                p_betas_momentum.append(model.pvalues[2])\n",
    "            r_squareds.append(model.rsquared)\n",
    "            # predict using the fitted model\n",
    "            if name == \"model1\":\n",
    "                pred = model.predict(sm.add_constant(sub_df1[[\"market_excess_return\", \"small_minus_big\"]]))\n",
    "                # calculate the MAE\n",
    "                MAE = np.mean(np.abs(sub_df1[var + \"_long_short\"] - sub_df1[\"DGS1MO\"] - pred))\n",
    "            elif name == \"model2\":\n",
    "                pred = model.predict(sm.add_constant(sub_df2[[\"market_excess_return\", \"momentum\"]]))\n",
    "                # calculate the MAE\n",
    "                MAE = np.mean(np.abs(sub_df2[var + \"_long_short\"] - sub_df2[\"DGS1MO\"] - pred))\n",
    "            elif name == \"model3\":\n",
    "                pred = model.predict(sm.add_constant(sub_df3[[\"market_excess_return\", \"small_minus_big\", \"momentum\"]]))\n",
    "                # calculate the MAE\n",
    "                MAE = np.mean(np.abs(sub_df3[var + \"_long_short\"] - sub_df3[\"DGS1MO\"] - pred))\n",
    "            mean_absolute_errors.append(MAE)\n",
    "            # comuting the average R-squared for the 5 quintiles excess return series as dependent variables\n",
    "            sum_r_squared = 0\n",
    "            if name == \"model1\":\n",
    "                for quintile in [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\"]:\n",
    "                    sub_df = df[[var + \"_\" + quintile, \"market_excess_return\", \"small_minus_big\"]].copy().dropna()\n",
    "                    model = sm.OLS(sub_df[var + \"_\" + quintile], sm.add_constant(sub_df[[\"market_excess_return\", \"small_minus_big\"]])).fit()\n",
    "                    sum_r_squared += model.rsquared\n",
    "            elif name == \"model2\":\n",
    "                for quintile in [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\"]:\n",
    "                    sub_df = df[[var + \"_\" + quintile, \"market_excess_return\", \"momentum\"]].copy().dropna()\n",
    "                    model = sm.OLS(sub_df[var + \"_\" + quintile], sm.add_constant(sub_df[[\"market_excess_return\", \"momentum\"]])).fit()\n",
    "                    sum_r_squared += model.rsquared\n",
    "            elif name == \"model3\":\n",
    "                for quintile in [\"q1\", \"q2\", \"q3\", \"q4\", \"q5\"]:\n",
    "                    sub_df = df[[var + \"_\" + quintile, \"market_excess_return\", \"small_minus_big\", \"momentum\"]].copy().dropna()\n",
    "                    model = sm.OLS(sub_df[var + \"_\" + quintile], sm.add_constant(sub_df[[\"market_excess_return\", \"small_minus_big\", \"momentum\"]])).fit()\n",
    "                    sum_r_squared += model.rsquared\n",
    "            average_r_squareds.append(sum_r_squared / 5)\n",
    "\n",
    "    df = pd.DataFrame({\"model\": models, \"ls_strategy\": variables, \"alpha\": alphas, \"t_alpha\": t_alphas, \"p_alpha\": p_alphas, \"beta_market_excess_return\": betas_market_excess_return, \"t_beta_market_excess_return\": t_betas_market_excess_return, \"p_beta_market_excess_return\": p_betas_market_excess_return, \"beta_small_minus_big\": betas_small_minus_big, \"t_beta_small_minus_big\": t_betas_small_minus_big, \"p_beta_small_minus_big\": p_betas_small_minus_big, \"beta_momentum\": betas_momentum, \"t_beta_momentum\": t_betas_momentum, \"p_beta_momentum\": p_betas_momentum, \"r_squared\": r_squareds, \"mean_absolute_error\": mean_absolute_errors, \"average_r_squared\": average_r_squareds})\n",
    "    # downloading the data\n",
    "    df.to_csv(directory + \"/multi_factor_models_data.csv\")\n",
    "else:\n",
    "    print(\"The data for the excess long-short strategies for the different volatility strategies has already been computed.\")\n",
    "    df = pd.read_csv(directory + \"/multi_factor_models_data.csv\")\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "\"\"\"\n",
    "# inverting the colors in the PDF in case the user is using dark mode\n",
    "if input(\"Is your editor is dark mode? y/n \") in [\"Y\", \"y\"]:\n",
    "    invert = True\n",
    "else:\n",
    "    invert = False\n",
    "\"\"\"\n",
    "invert = True\n",
    "\n",
    "helpers.render_factor_models_statistics(df, \"latex_templates/multi_factor_models_statistics.tex\", invert)\n",
    "\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74309577635f6089c540d7d4ca5dc414c3665bebb3159a664b0224275d6afd7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
