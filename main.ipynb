{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Risk Factors in Cryptocurrency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis Project\n",
    "\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0</div>\n",
    "\n",
    "This file requires `pandas`, `datetime`, `numpy`, `wand`, `pdf2image`, `Pillow`, and `math` to run. If one of these imports fails, please install the corresponding library and make sure that you have activated the corresponding virtual environment.\n",
    "\n",
    "The project follows closely the methodology proposed by Liu, Tsyvinski, and Wu (2022) in their paper titled [Common Risk Factors in Cryptocurrency](https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13119). Researchers and practitioners can use this paper to check the results of the paper and perhaps retrieve an updated version of the basic findings. They can also use it as a toolbox to use for other projects or to run an extended analysis including further risk factors. Finally, asset management firm may use this code to assess the risk of their portfolio or to firm anomalies in the returns of cryptocurrencies.\n",
    "\n",
    "For this analysis, I occasionally had to make assumption, for example, regarding the procedure to convert daily to weekly data. This is especially so because the authors of the paper did not provide a detailed enough description of their decisions. There are other, perhabs better ways of doing certain steps and I am always grateful for any feedback that you might provide.\n",
    "\n",
    "The order of the following sections is closely following the structure of the paper. The outline is:\n",
    "* [I. Data](#I.-Data): The files for all data sources can be found in the data folder. The main blockchain trading data is retrieved from CoinGecko (coingecko_data.py). It is advisable to download the cryptocurrency data set in smaller chunks (for example, 100 cryptocurrencies), since the data set is relatively large and takes long to download due to the API limit. The merge_data.py file can then be used to merge all individal cryptocurrency data files into one large file that is supposed to be loaded into the code below. The daily crptocurrency (aka coin) data is converted to weekly returns using the last available prices: $$r_t = \\frac{p_t-p_{t-1}}{p_{t-1}}$$ One can also compute log-returns instead. The definition of the weeks is as follows: The first 7 days of a given year are the first week. The following 50 weeks consist of 7 days each. The last week has either 8 or 9 days (if the year is a leap year). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individual data files already exist.\n",
      "The data was already merged into a single file.\n",
      "There are 1812 coins in the data set.\n",
      "The data has already been converted.\n",
      "The data is being transformed into weekly returns data.\n",
      "The conversion progress is: \n",
      "2%\n",
      "3%\n",
      "4%\n",
      "5%\n",
      "6%\n",
      "7%\n",
      "8%\n",
      "9%\n",
      "10%\n",
      "11%\n",
      "12%\n",
      "13%\n",
      "14%\n",
      "15%\n",
      "16%\n",
      "17%\n",
      "18%\n",
      "19%\n",
      "20%\n",
      "21%\n",
      "22%\n",
      "23%\n",
      "24%\n",
      "25%\n",
      "26%\n",
      "27%\n",
      "28%\n",
      "29%\n",
      "30%\n",
      "31%\n",
      "32%\n",
      "33%\n",
      "34%\n",
      "35%\n",
      "36%\n",
      "37%\n",
      "38%\n",
      "39%\n",
      "40%\n",
      "41%\n",
      "42%\n",
      "43%\n",
      "44%\n",
      "45%\n",
      "46%\n",
      "47%\n",
      "48%\n",
      "49%\n",
      "50%\n",
      "51%\n",
      "52%\n",
      "53%\n",
      "54%\n",
      "55%\n",
      "56%\n",
      "57%\n",
      "58%\n",
      "59%\n",
      "60%\n",
      "61%\n",
      "62%\n",
      "63%\n",
      "64%\n",
      "65%\n",
      "66%\n",
      "67%\n",
      "68%\n",
      "69%\n",
      "70%\n",
      "71%\n",
      "72%\n",
      "73%\n",
      "74%\n",
      "75%\n",
      "76%\n",
      "77%\n",
      "78%\n",
      "79%\n",
      "80%\n",
      "81%\n",
      "82%\n",
      "83%\n",
      "84%\n",
      "85%\n",
      "86%\n",
      "87%\n",
      "88%\n",
      "89%\n",
      "90%\n",
      "91%\n",
      "92%\n",
      "93%\n",
      "94%\n",
      "95%\n",
      "96%\n",
      "97%\n",
      "98%\n",
      "99%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 141\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mfor\u001b[39;00m coin_id \u001b[39min\u001b[39;00m coin_ids:\n\u001b[1;32m    140\u001b[0m     coin_weekly_returns \u001b[39m=\u001b[39m coins_weekly_returns[coin_id]\n\u001b[0;32m--> 141\u001b[0m     coin_weekly_data \u001b[39m=\u001b[39m coin_weekly_returns[(coin_weekly_returns[\u001b[39m\"\u001b[39;49m\u001b[39myear\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m year) \u001b[39m&\u001b[39;49m (coin_weekly_returns[\u001b[39m\"\u001b[39;49m\u001b[39mweek\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m==\u001b[39;49m week)]\n\u001b[1;32m    142\u001b[0m     \u001b[39m# ignoring all NaNs\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39m# the most convenient way to check if no cell value are NaN is by applying .isna().sum().sum()\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m coin_weekly_data\u001b[39m.\u001b[39misna()\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39msum() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[39m# the ID is included\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/pandas/core/frame.py:3795\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhere(key)\n\u001b[1;32m   3794\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m-> 3795\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39;49mis_bool_indexer(key):\n\u001b[1;32m   3796\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_bool_array(key)\n\u001b[1;32m   3798\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/pandas/core/common.py:127\u001b[0m, in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39mCheck whether `key` is a valid boolean indexer.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m    and convert to an ndarray.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, (ABCSeries, np\u001b[39m.\u001b[39mndarray, ABCIndex)) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m     is_array_like(key) \u001b[39mand\u001b[39;00m is_extension_array_dtype(key\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    126\u001b[0m ):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39;49mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mobject_:\n\u001b[1;32m    128\u001b[0m         key_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key)\n\u001b[1;32m    130\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39mis_bool_array(key_array):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/pandas/core/series.py:596\u001b[0m, in \u001b[0;36mSeries.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mset_axis(axis, labels)\n\u001b[1;32m    595\u001b[0m \u001b[39m# ndarray compatibility\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DtypeObj:\n\u001b[1;32m    598\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m    Return the dtype object of the underlying data.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mdtype\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd, datetime, numpy as np, math, time, os\n",
    "pd.options.mode.chained_assignment = None\n",
    "from wand.image import Image\n",
    "# and other modules from the directory\n",
    "import convert_frequency, data.coingecko_data as coingecko_data, merge_data\n",
    "# render_df, \n",
    "\n",
    "# specify the data range for the analysis\n",
    "# in the paper, the authors start on 2014-01-01 due to data availability\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = str(datetime.date.today())\n",
    "\n",
    "# select the path to the directory where you want to store the data\n",
    "data_path = r\"/Users/Marc/Desktop/Past Affairs/Past Universities/SSE Courses/Master Thesis/Data\"\n",
    "\n",
    "# downloading the data from CoinGecko.com and storing it in smaller data subsets at the specified location\n",
    "# the data contains daily prices, market caps, and trading volumes\n",
    "# this step can take up to 2 days due to the API traffic limit\n",
    "# we are also always checking if the subsequent files already exist (/cg_data.csv, /cg_weekly_data.csv, /cg_weekly_returns.csv, /market_weekly_returns.csv)\n",
    "# this helps in case the previous files have been deleted\n",
    "if not os.path.exists(data_path + \"/coingecko\") and not os.path.exists(data_path + \"/cg_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    coingecko_data.retrieve_data(start_date, end_date, path=data_path)\n",
    "else:\n",
    "    print(\"The individual data files already exist.\")\n",
    "\n",
    "# merging the data subsets and storing the result at the specified location\n",
    "# this task also absorbs part of the preprocessing, so it's recommended to run this step in any case \n",
    "# this step can take up to 12 hours\n",
    "if not os.path.exists(data_path + \"/cg_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    merge_data.merge(start_date, end_date, path=data_path)\n",
    "else:\n",
    "    print(\"The data was already merged into a single file.\")\n",
    "\n",
    "# the data was retrieved on 2023-01-13\n",
    "daily_trading_data = pd.read_csv(data_path+\"/cg_data.csv\")\n",
    "\n",
    "# all unique coin IDs\n",
    "coin_ids = pd.unique(daily_trading_data[\"id\"])\n",
    "print(\"There are \" +str(len(coin_ids)) + \" coins in the data set.\")\n",
    "\n",
    "# downloading the data since the conversion process might also take a long time\n",
    "# if the file for weekly data does not already exist\n",
    "if not os.path.exists(data_path + \"/cg_weekly_data.csv\") and not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    print(\"The data is being transformed into weekly data.\")\n",
    "    # converting the data subset for every coin into weekly frequency\n",
    "    dfs = []\n",
    "    percentage_counter = 1\n",
    "    print(\"The conversion progress is: \")\n",
    "    for coin_id in coin_ids:\n",
    "        # printing the progress \n",
    "        progress = int(len(dfs) / len(coin_ids) * 100)\n",
    "        if progress > percentage_counter:\n",
    "            percentage_counter += 1\n",
    "            print(str(progress) + \"%\")\n",
    "        # get all the data for one coin\n",
    "        coin_daily_trading_data = daily_trading_data[daily_trading_data[\"id\"] == coin_id]\n",
    "        # now we compute the weekly data\n",
    "        # the function weekly_data is designed to perform this transformation for a single coin at a time\n",
    "        # this step takes a long time since the data set has a large size\n",
    "        coin_weekly_data = convert_frequency.weekly_data(coin_daily_trading_data, start_date, end_date, download=False)\n",
    "        dfs.append(coin_weekly_data)\n",
    "        \n",
    "    # combining all dataframes in the dfs list\n",
    "    weekly_trading_data = pd.concat(dfs)\n",
    "    # downloading the data\n",
    "    weekly_trading_data.to_csv(data_path + \"/cg_weekly_data.csv\", index=False)\n",
    "else:\n",
    "    # next, we need to load the data and \"unwrap\" it again\n",
    "    print(\"The data has already been converted.\")\n",
    "    weekly_trading_data = pd.read_csv(data_path + \"/cg_weekly_data.csv\")\n",
    "\n",
    "# downloading the data since the returns computation process might also take a long time\n",
    "# if the file for weekly returns data does not already exist\n",
    "if not os.path.exists(data_path + \"/cg_weekly_returns.csv\") and not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    print(\"The data is being transformed into weekly returns data.\")\n",
    "    # converting the data subset for every coin into weekly frequency\n",
    "    dfs = []\n",
    "    percentage_counter = 1\n",
    "    print(\"The conversion progress is: \")\n",
    "    for coin_id in coin_ids:\n",
    "        # printing the progress \n",
    "        progress = int(len(dfs) / len(coin_ids) * 100)\n",
    "        if progress > percentage_counter:\n",
    "            percentage_counter += 1\n",
    "            print(str(progress) + \"%\")\n",
    "        coin_weekly_prices = weekly_trading_data[weekly_trading_data[\"id\"] == coin_id]\n",
    "        # we are losing the first week, since we do not have a previous week for the first week (first week of 2014)\n",
    "        coin_weekly_returns = [np.nan]\n",
    "        # the indices of the dataframe for the respective coin\n",
    "        indices = list(coin_weekly_prices.index)\n",
    "        if len(coin_weekly_prices) > 2:\n",
    "            for i in range(len(coin_weekly_prices) - 1):\n",
    "                # to retrieve the actual indices\n",
    "                this_week_index = indices[i]\n",
    "                next_week_index = indices[i + 1]\n",
    "                try:\n",
    "                    weekly_return = (coin_weekly_prices[\"price\"][next_week_index] - coin_weekly_prices[\"price\"][this_week_index]) / coin_weekly_prices[\"price\"][this_week_index]\n",
    "                    # alternatively, the log-return can be computed as follows (math.log() is the natural logarithm by default):\n",
    "                    # weekly_log_return = math.log(coin_weekly_prices[\"price\"][i + 1] / coin_weekly_prices[\"price\"][i])\n",
    "                    coin_weekly_returns.append(weekly_return)\n",
    "                except:\n",
    "                    # this exception occurs when either current or future price are NaN\n",
    "                    coin_weekly_returns.append(np.nan)\n",
    "        # adding the return column to the previous date column\n",
    "        coin_weekly_prices[\"return\"] = coin_weekly_returns\n",
    "        dfs.append(coin_weekly_prices)\n",
    "    # combining all dataframes in the dfs list\n",
    "    coins_weekly_returns = pd.concat(dfs)\n",
    "    # downloading the data\n",
    "    coins_weekly_returns.to_csv(data_path + \"/cg_weekly_returns_data.csv\", index=False)\n",
    "else:\n",
    "    # next, we need to load the data and \"unwrap\" it again\n",
    "    print(\"The data has already been transformed into returns data.\")\n",
    "    coins_weekly_returns = pd.read_csv(data_path + \"/cg_weekly_returns.csv\")\n",
    "\n",
    "coins_weekly_returns_old = coins_weekly_returns\n",
    "# storing the data in a dict with keys for the ID\n",
    "coins_weekly_returns = {}\n",
    "for coin_id in coin_ids:\n",
    "    # get all the data for one coin\n",
    "    coins_weekly_returns[coin_id] = coins_weekly_returns_old[coins_weekly_returns_old[\"id\"] == coin_id]\n",
    "\n",
    "# downloading the data since the returns computation process might also take a long time\n",
    "# if the file for weekly returns data does not already exist\n",
    "if not os.path.exists(data_path + \"/market_weekly_returns.csv\"):\n",
    "    # constructing the cryptocurrency market returns\n",
    "    years = []\n",
    "    weeks = []\n",
    "    market_returns = []\n",
    "    included_ids = []\n",
    "    # taking an arbitrary ID to loop through all weeks\n",
    "    for i in coins_weekly_returns[coin_ids[0]].index:\n",
    "        year = coins_weekly_returns[coin_ids[0]][\"year\"][i]\n",
    "        years.append(year)\n",
    "        week = coins_weekly_returns[coin_ids[0]][\"week\"][i]\n",
    "        weeks.append(week)\n",
    "        returns = []\n",
    "        market_caps = []\n",
    "        # to keep track of which and how many coins are included for every week\n",
    "        weekly_included_ids = []\n",
    "        for coin_id in coin_ids:\n",
    "            coin_weekly_returns = coins_weekly_returns[coin_id]\n",
    "            coin_weekly_data = coin_weekly_returns[(coin_weekly_returns[\"year\"] == year) & (coin_weekly_returns[\"week\"] == week)]\n",
    "            # ignoring all NaNs\n",
    "            # the most convenient way to check if no cell value are NaN is by applying .isna().sum().sum()\n",
    "            if coin_weekly_data.isna().sum().sum() == 0:\n",
    "                # the ID is included\n",
    "                weekly_included_ids.append(coin_id)\n",
    "                returns.append(coin_weekly_data[\"return\"])\n",
    "                market_caps.append(coin_weekly_data[\"market_cap\"])\n",
    "        # if all returns are NaN (for example, in the first week of the time period considered)\n",
    "        if len(returns) == 0:\n",
    "            # if no value was added\n",
    "            market_returns.append(np.nan)\n",
    "            included_ids.append(np.nan)\n",
    "        else:\n",
    "            # for every week add the value-weighted market return (the sumproduct of the returns and the market caps divided by the sum of the market caps) and the included coin IDs\n",
    "            weighted_averge = (sum(x * y for x, y in zip(returns, market_caps)) / sum(market_caps)).tolist()[0]\n",
    "            market_returns.append(weighted_averge)\n",
    "            included_ids.append(weekly_included_ids)\n",
    "    market_weekly_returns = pd.DataFrame({\"year\": years, \"week\": weeks, \"average_return\": market_returns, \"included_ids\": included_ids})\n",
    "    # downloading the data\n",
    "    coins_weekly_returns.to_csv(data_path + \"/market_weekly_returns.csv\", index=False)\n",
    "else:\n",
    "    # next, we need to load the data and \"unwrap\" it again\n",
    "    print(\"The data market returns have already been computred.\")\n",
    "    market_weekly_returns = pd.read_csv(data_path + \"/market_weekly_returns.csv\")\n",
    "\n",
    "print(market_weekly_returns.head())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "render_df.render_summary_statistics(start_date, end_date, daily_trading_data, market_returns)\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74309577635f6089c540d7d4ca5dc414c3665bebb3159a664b0224275d6afd7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
