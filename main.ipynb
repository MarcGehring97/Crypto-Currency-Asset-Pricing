{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Risk Factors in Cryptocurrency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis Project\n",
    "\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0</div>\n",
    "\n",
    "This file requires `pandas`, `datetime`, `numpy`, and `math` to run. If one of these imports fails, please install the corresponding library and make sure that you have activated the corresponding virtual environment.\n",
    "\n",
    "The project follows closely the methodology proposed by Liu, Tsyvinski, and Wu (2022) in their paper titled [Common Risk Factors in Cryptocurrency](https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13119). Researchers and practitioners can use this paper to check the results of the paper and perhaps retrieve an updated version of the basic findings. They can also use it as a toolbox to use for other projects or to run an extended analysis including further risk factors. Finally, asset management firm may use this code to assess the risk of their portfolio or to firm anomalies in the returns of cryptocurrencies.\n",
    "\n",
    "For this analysis, I occasionally had to make assumption, for example, regarding the procedure to convert daily to weekly data. This is especially so because the authors of the paper did not provide a detailed enough description of their decisions. There are other, perhabs better ways of doing certain steps and I am always grateful for any feedback that you might provide.\n",
    "\n",
    "The order of the following sections is closely following the structure of the paper. The outline is:\n",
    "* [I. Data](#I.-Data): The files for all data sources can be found in the data folder. The main blockchain trading data is retrieved from CoinGecko (coingecko_data.py). It is advisable to download the cryptocurrency data set in smaller chunks (for example, 100 cryptocurrencies), since the data set is relatively large and takes long to download due to the API limit. The merge_data.py file can then be used to merge all individal cryptocurrency data files into one large file that is supposed to be loaded into the code below. The daily crptocurrency (aka coin) data is converted to weekly returns using the last available prices: $$r_t = \\frac{p_t-p_{t-1}}{p_{t-1}}$$ One can also compute log-returns instead. The definition of the weeks is as follows: The first 7 days of a given year are the first week. The following 50 weeks consist of 7 days each. The last week has either 8 or 9 days (if the year is a leap year). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, datetime, convert_frequency as cf, numpy as np, math\n",
    "\n",
    "# specify the data range for the analysis\n",
    "# in the paper, the authors start on 2014-01-01 due to data availability\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = str(datetime.date.today())\n",
    "\n",
    "# the path where you have stored the cryptocurrency trading data\n",
    "data_path = \"/Users/Marc/Desktop/Past Affairs/Past Universities/SSE Courses/Master Thesis/Data\"\n",
    "# daily price, market cap, and trading volume data\n",
    "daily_trading_data = pd.read_csv(data_path + \"/coingecko/cg_data.csv\")\n",
    "\n",
    "# constructing weekly coin returns\n",
    "# all unique coin IDs\n",
    "coin_ids = pd.unique(daily_trading_data[\"id\"])\n",
    "# storing the data in a list with tuples for the ID and the weekly prices (last available prices each week)\n",
    "coins_weekly_prices = {}\n",
    "for coin_id in coin_ids:\n",
    "    # get all the data for one coin\n",
    "    coin_daily_trading_data = daily_trading_data[daily_trading_data[\"id\"] == coin_id]\n",
    "    # we need market_cap to compute the value-weighted market returns\n",
    "    coin_daily_prices = coin_daily_trading_data[[\"date\", \"price\", \"market_cap\"]]\n",
    "    # now we compute the weekly prices\n",
    "    # the first column is year/week and the second column is the weekly price\n",
    "    coin_weekly_prices = cf.weekly_data(coin_daily_prices)\n",
    "    coins_weekly_prices[coin_id] = coin_weekly_prices\n",
    "\n",
    "# using the weekly price data to compute the weekly returns\n",
    "coins_weekly_returns = {}\n",
    "for coin_id in coin_ids:\n",
    "    coin_weekly_prices = coins_weekly_prices[coin_id]\n",
    "    # we are losing the first week, since we do not have a previous week for the first week (first week of 2014)\n",
    "    coin_weekly_returns = [np.nan]\n",
    "    for i in range(len(coin_weekly_prices) - 1):\n",
    "        weekly_return = (coin_weekly_prices[\"price\"][i + 1] - coin_weekly_prices[\"price\"][i]) / coin_weekly_prices[\"price\"][i]\n",
    "        # alternatively, the log-return can be computed as follows:\n",
    "        # weekly_log_return = math.log(coin_weekly_prices[\"price\"][i + 1] / coin_weekly_prices[\"price\"][i])\n",
    "        coin_weekly_returns.append(weekly_return)\n",
    "    # adding the return column to the previous date column\n",
    "    df = coin_weekly_prices[\"year/week\", \"market_cap\"]\n",
    "    df[\"return\"] = coin_weekly_returns\n",
    "    coins_weekly_returns[coin_id] = df\n",
    "    # checking for NaNs\n",
    "    if df[\"return\"].isna().sum() != 0:\n",
    "        print(\"The number of NaNs entries for \" + coin_id + \" is: \" + str(df[\"return\"].isna().sum()))\n",
    "\n",
    "# constructing the cryptocurrency market returns\n",
    "weeks = []\n",
    "market_returns = []\n",
    "included_ids = []\n",
    "# looping through all weeks\n",
    "for week in coins_weekly_returns[coin_ids[0]][\"year/week\"]:\n",
    "    weeks.append(week)\n",
    "    returns = []\n",
    "    market_caps = []\n",
    "    # to keep track of which and how many coins are included for every week\n",
    "    weekly_included_ids = []\n",
    "    for coin_id in coin_ids:\n",
    "        coin_weekly_returns = coins_weekly_returns[coin_id]\n",
    "        coin_weekly_data = coin_weekly_returns[coin_weekly_returns[\"year/week\"] == week]\n",
    "        # ignoring all NaNs\n",
    "        # the most convenient way to check if no cell value are NaN is by applying .isna().sum().sum()\n",
    "        if coin_weekly_data.isna().sum().sum() == 0:\n",
    "            # the ID is included\n",
    "            weekly_included_ids.append(coin_id)\n",
    "            returns.append(coin_weekly_data[\"return\"])\n",
    "            market_caps.append(coin_weekly_data[\"market_cap\"])\n",
    "    # if all returns are NaN (for example, in the first week of the time period considered)\n",
    "    if len(returns) == 0:\n",
    "        # no value is added\n",
    "        market_returns.append(np.nan)\n",
    "        included_ids.append(np.nan)\n",
    "    else:\n",
    "        # for every week add the value-weighted market return (the sumproduct of the returns and the market caps divided by the sum of the market caps) and the included coin IDs\n",
    "        market_returns.append(sum(x * y for x, y in zip(returns, market_caps)) / sum(market_caps))\n",
    "        included_ids.append(weekly_included_ids)\n",
    "market_returns = pd.DataFrame({\"week\": weeks, \"average_return\": market_returns, \"included_ids\": included_ids})\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (main, Dec 15 2022, 18:18:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
