{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Risk Factors in Cryptocurrency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Thesis Project\n",
    "\n",
    "<div style=\"text-align: right;font-size: 0.8em\">Document Version 1.0.0</div>\n",
    "\n",
    "This file requires `pandas`, `datetime`, `numpy`, `wand`, `pdf2image`, `Pillow`, and `math` to run. If one of these imports fails, please install the corresponding library and make sure that you have activated the corresponding virtual environment.\n",
    "\n",
    "The project follows closely the methodology proposed by Liu, Tsyvinski, and Wu (2022) in their paper titled [Common Risk Factors in Cryptocurrency](https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.13119). Researchers and practitioners can use this paper to check the results of the paper and perhaps retrieve an updated version of the basic findings. They can also use it as a toolbox to use for other projects or to run an extended analysis including further risk factors. Finally, asset management firm may use this code to assess the risk of their portfolio or to firm anomalies in the returns of cryptocurrencies.\n",
    "\n",
    "For this analysis, I occasionally had to make assumption, for example, regarding the procedure to convert daily to weekly data. This is especially so because the authors of the paper did not provide a detailed enough description of their decisions. There are other, perhabs better ways of doing certain steps and I am always grateful for any feedback that you might provide.\n",
    "\n",
    "The order of the following sections is closely following the structure of the paper. The outline is:\n",
    "* [I. Data](#I.-Data): The files for all data sources can be found in the data folder. The main blockchain trading data is retrieved from CoinGecko (coingecko_data.py). It is advisable to download the cryptocurrency data set in smaller chunks (for example, 100 cryptocurrencies), since the data set is relatively large and takes long to download due to the API limit. The merge_data.py file can then be used to merge all individal cryptocurrency data files into one large file that is supposed to be loaded into the code below. The daily crptocurrency (aka coin) data is converted to weekly returns using the last available prices: $$r_t = \\frac{p_t-p_{t-1}}{p_{t-1}}$$ One can also compute log-returns instead. The definition of the weeks is as follows: The first 7 days of a given year are the first week. The following 50 weeks consist of 7 days each. The last week has either 8 or 9 days (if the year is a leap year). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individual data files already exist.\n",
      "The data was already merged into a single file.\n",
      "The data has already been converted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cn/dd2qmn7d0_7_sh8vgy3mg1pm0000gn/T/ipykernel_97177/512830422.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  coin_weekly_prices[\"return\"] = coin_weekly_returns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of NaNs entries for stacktical is: 321\n",
      "stader\n",
      "[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565, 0.6418810543019565]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (1) does not match length of index (468)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mlist\u001b[39m(coin_weekly_prices[\u001b[39m\"\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[1;32m     90\u001b[0m \u001b[39m# adding the return column to the previous date column\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m coin_weekly_prices[\u001b[39m\"\u001b[39;49m\u001b[39mreturn\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m coin_weekly_returns\n\u001b[1;32m     92\u001b[0m coins_weekly_returns[coin_id] \u001b[39m=\u001b[39m coin_weekly_prices\n\u001b[1;32m     93\u001b[0m \u001b[39m# checking for NaNs\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/pandas/core/frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/pandas/core/frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4163\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4178\u001b[0m     ):\n\u001b[1;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/pandas/core/frame.py:4912\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4909\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4911\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 4912\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   4913\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/pandas/core/common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (1) does not match length of index (468)"
     ]
    }
   ],
   "source": [
    "import pandas as pd, datetime, numpy as np, math, time, os\n",
    "from wand.image import Image\n",
    "# and other modules from the directory\n",
    "import convert_frequency, data.coingecko_data as coingecko_data, merge_data\n",
    "# render_df, \n",
    "\n",
    "# specify the data range for the analysis\n",
    "# in the paper, the authors start on 2014-01-01 due to data availability\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = str(datetime.date.today())\n",
    "\n",
    "# select the path to the directory where you want to store the data\n",
    "data_path = r\"/Users/Marc/Desktop/Past Affairs/Past Universities/SSE Courses/Master Thesis/Data\"\n",
    "\n",
    "# downloading the data from CoinGecko.com and storing it in smaller data subsets at the specified location\n",
    "# the data contains daily prices, market caps, and trading volumes\n",
    "# this step can take up to 2 days due to the API traffic limit\n",
    "if not os.path.exists(data_path + \"/coingecko\"):\n",
    "    coingecko_data.retrieve_data(start_date, end_date, path=data_path)\n",
    "else:\n",
    "    print(\"The individual data files already exist.\")\n",
    "\n",
    "# merging the data subsets and storing the result at the specified location\n",
    "# this task also absorbs part of the preprocessing, so it's recommended to run this step in any case \n",
    "# this step can take up to 12 hours\n",
    "if not os.path.exists(data_path + \"/cg_data.csv\"):\n",
    "    merge_data.merge(start_date, end_date, path=data_path)\n",
    "else:\n",
    "    print(\"The data was already merged into a single file.\")\n",
    "\n",
    "# the data was retrieved on 2023-01-13\n",
    "daily_trading_data = pd.read_csv(data_path+\"/cg_data.csv\")\n",
    "\n",
    "# all unique coin IDs\n",
    "coin_ids = pd.unique(daily_trading_data[\"id\"])\n",
    "\n",
    "# downloading the data since the conversion process might also take a long time\n",
    "# if the file for weekly data does not already exist\n",
    "if not os.path.exists(data_path + \"/cg_weekly_data.csv\"):\n",
    "    print(\"The data is now transformed into weekly data.\")\n",
    "    # converting the data subset for every coin into weekly frequency\n",
    "    dfs = []\n",
    "    percentage_counter = 1\n",
    "    print(\"The conversion progress is: \")\n",
    "    for coin_id in coin_ids:\n",
    "        # printing the progress \n",
    "        progress = int(len(dfs) / len(coin_ids) * 100)\n",
    "        if progress > percentage_counter:\n",
    "            percentage_counter += 1\n",
    "            print(str(progress) + \"%\")\n",
    "        # get all the data for one coin\n",
    "        coin_daily_trading_data = daily_trading_data[daily_trading_data[\"id\"] == coin_id]\n",
    "        # now we compute the weekly data\n",
    "        # the function weekly_data is designed to perform this transformation for a single coin at a time\n",
    "        # this step takes a long time since the data set has a large size\n",
    "        coin_weekly_data = convert_frequency.weekly_data(coin_daily_trading_data, start_date, end_date, download=False)\n",
    "        dfs.append(coin_weekly_data)\n",
    "        \n",
    "    # combining all dataframes in the dfs list\n",
    "    weekly_trading_data = pd.concat(dfs)\n",
    "    # downloading the data\n",
    "    weekly_trading_data.to_csv(data_path + \"/cg_weekly_data.csv\", index=False)\n",
    "else:\n",
    "    # next, we need to load the data and \"unwrap\" it again\n",
    "    print(\"The data has already been converted.\")\n",
    "    weekly_trading_data = pd.read_csv(data_path+\"/cg_weekly_data.csv\")\n",
    "\n",
    "# storing the data in a dict with keys for the ID\n",
    "coins_weekly_prices = {}\n",
    "for coin_id in coin_ids:\n",
    "    # get all the data for one coin\n",
    "    coins_weekly_prices[coin_id] = weekly_trading_data[weekly_trading_data[\"id\"] == coin_id]\n",
    "\n",
    "# using the weekly price data to compute the weekly returns\n",
    "coins_weekly_returns = {}\n",
    "for coin_id in coin_ids:\n",
    "    coin_weekly_prices = coins_weekly_prices[coin_id]\n",
    "    # we are losing the first week, since we do not have a previous week for the first week (first week of 2014)\n",
    "    coin_weekly_returns = [np.nan]\n",
    "    if len(coin_weekly_prices) > 2:\n",
    "        for i in range(len(coin_weekly_prices) - 1):\n",
    "            try:\n",
    "                weekly_return = (coin_weekly_prices[\"price\"][i + 1] - coin_weekly_prices[\"price\"][i]) / coin_weekly_prices[\"price\"][i]\n",
    "                # alternatively, the log-return can be computed as follows (math.log() is the natural logarithm by default):\n",
    "                # weekly_log_return = math.log(coin_weekly_prices[\"price\"][i + 1] / coin_weekly_prices[\"price\"][i])\n",
    "                coin_weekly_returns.append(weekly_return)\n",
    "            except:\n",
    "                # this exception occurs when either current or future price are NaN\n",
    "                coin_weekly_returns.append(np.nan)\n",
    "    # adding the return column to the previous date column\n",
    "    coin_weekly_prices[\"return\"] = coin_weekly_returns\n",
    "    coins_weekly_returns[coin_id] = coin_weekly_prices\n",
    "    # checking for NaNs\n",
    "    if coin_weekly_prices[\"return\"].isna().sum() != 0:\n",
    "        print(\"The number of NaNs entries for \" + coin_id + \" is: \" + str(coin_weekly_prices[\"return\"].isna().sum()))\n",
    "\n",
    "# constructing the cryptocurrency market returns\n",
    "years = []\n",
    "weeks = []\n",
    "market_returns = []\n",
    "included_ids = []\n",
    "# looping through all weeks\n",
    "for i in coins_weekly_returns[coin_ids[0]].index:\n",
    "    year = coins_weekly_returns[coin_ids[0]][\"year\"][i]\n",
    "    years.append(year)\n",
    "    week = coins_weekly_returns[coin_ids[0]][\"week\"][i]\n",
    "    weeks.append(week)\n",
    "    returns = []\n",
    "    market_caps = []\n",
    "    # to keep track of which and how many coins are included for every week\n",
    "    weekly_included_ids = []\n",
    "    for coin_id in coin_ids:\n",
    "        coin_weekly_returns = coins_weekly_returns[coin_id]\n",
    "        coin_weekly_data = coin_weekly_returns[(coin_weekly_returns[\"year\"] == year) & (coin_weekly_returns[\"week\"] == week)]\n",
    "        # ignoring all NaNs\n",
    "        # the most convenient way to check if no cell value are NaN is by applying .isna().sum().sum()\n",
    "        if coin_weekly_data.isna().sum().sum() == 0:\n",
    "            # the ID is included\n",
    "            weekly_included_ids.append(coin_id)\n",
    "            returns.append(coin_weekly_data[\"return\"])\n",
    "            market_caps.append(coin_weekly_data[\"market_cap\"])\n",
    "    # if all returns are NaN (for example, in the first week of the time period considered)\n",
    "    if len(returns) == 0:\n",
    "        # no value is added\n",
    "        market_returns.append(np.nan)\n",
    "        included_ids.append(np.nan)\n",
    "    else:\n",
    "        # for every week add the value-weighted market return (the sumproduct of the returns and the market caps divided by the sum of the market caps) and the included coin IDs\n",
    "        market_returns.append(sum(x * y for x, y in zip(returns, market_caps)) / sum(market_caps))\n",
    "        included_ids.append(weekly_included_ids)\n",
    "market_returns = pd.DataFrame({\"year\": years, \"week\": weeks, \"average_return\": market_returns, \"included_ids\": included_ids})\n",
    "\n",
    "print(market_returns.head())\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# creates a temporary PDF file named \"cover.pdf\"\n",
    "# repeating the process overwrites the file\n",
    "render_df.render_summary_statistics(start_date, end_date, daily_trading_data, market_returns)\n",
    "\n",
    "pdf_path = os.getcwd() + \"/cover.pdf\"\n",
    "# printing the PDF\n",
    "# this code has to be in the main file\n",
    "img = Image(filename=pdf_path, resolution=100)\n",
    "img\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74309577635f6089c540d7d4ca5dc414c3665bebb3159a664b0224275d6afd7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
